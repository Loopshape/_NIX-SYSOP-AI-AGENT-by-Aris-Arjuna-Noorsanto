#!/usr/bin/env node

import readline from 'readline';
import path from 'path'; 
// !!! NOTE: UNCOMMENT THE FOLLOWING LINES IN YOUR LOCAL NODE.JS ENVIRONMENT !!!
// import { spawn } from 'child_process'; 
// import fetch from 'node-fetch'; 
// -------------------------------------------------------------------------

// --- CONFIGURATION AND UTILITIES ---

const OLLAMA_BIN = process.env.OLLAMA_BIN || 'ollama';
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || 'YOUR_OPENAI_API_KEY';

// Your dedicated Ollama models for the batch process
const DEDICATED_MODELS = ['loop', '2244', 'code', 'core', 'coin'];

const MOCK_FILE_CONTENTS = {
    'src/config.py': 'def initialize(): # High-priority setup.',
    'src/data.ts': 'const data = loadExternalData("api/v2");',
    'docs/report.md': '# Final Report Generation. Summarize all data.',
    'internal/setup.txt': 'Initial environment configuration parameters.',
    'data/metrics.json': '{"value": 42.0, "unit": "entropy"}' 
};
const MOCK_URL_CONTENT = 'Dynamic external reference: This content is key to reconciliation.';

/** Maps file extensions/paths to your dedicated Ollama models. */
function assignModel(filePath) {
    if (filePath.endsWith('.py') || filePath.endsWith('.ts')) return 'code';
    if (filePath.endsWith('.md')) return 'coin'; 
    if (filePath.includes('config') || filePath.includes('setup')) return 'core';
    if (filePath.includes('data')) return '2244';
    return 'loop';
}

function getLabelColor(label) {
    const hash = label.split('').reduce((acc, char) => char.charCodeAt(0) + acc, 0);
    const colorCode = 31 + (hash % 6); 
    return `\x1b[1;${colorCode}m`;
}

function parseFlags(args) {
    const flags = {};
    args.forEach(arg => {
        // Handle command without a flag, e.g., 'dialog'
        if (arg && !arg.startsWith('--')) return;
        
        if (arg.startsWith('--')) {
            const [key, val] = arg.slice(2).split('=');
            flags[key] = val === undefined ? true : val;
        }
    });
    return flags;
}

// --- BACKEND STREAMING IMPLEMENTATION (MOCKED/SIMULATED) ---

/**
 * Executes a prompt against an LLM backend and streams the output.
 * @returns {Promise<string>} The full, final response string.
 */
async function streamBackendResponse(model, prompt, prefix = '') {
    // --- REAL BACKEND STREAMING CODE GOES HERE ---
    /* Uncomment Ollama/OpenAI code from the previous response here.
    */
    // --- END REAL BACKEND STREAMING CODE ---

    // --- MOCKED STREAMING FALLBACK ---
    const promptId = prompt.substring(0, 10).trim();
    const responseText = `(MOCK: ${model}) Consensus check initiated. Processing request [${promptId}]. Resulting fusion stream...`;
    
    // Simulate token streaming
    const tokens = responseText.split(/(\s+)/).filter(t => t.length > 0);
    let output = '';
    
    for (const token of tokens) {
        process.stdout.write(prefix + token.trim());
        output += token;
        await new Promise(r => setTimeout(r, 5)); 
    }
    process.stdout.write('\n');
    return output.trim();
}


// --- UNIFIED META-CONSENSUS ENGINE ---

class UnifiedMetaEngine {
    constructor({files=[], models=['core'], backend='ollama', apiKey='', maxIter=8, urlContent=''}) {
        this.files = files;
        this.models = models;
        this.backend = backend;
        this.apiKey = apiKey;
        this.maxIter = maxIter;
        this.urlContent = urlContent;
        
        this.states = {}; 
        this.fusedOutput = '';
        this.lastSignature = '';
        this.currentIter = 0;
        
        // Dynamic state initialization based on mode
        if (files.length) {
            // Batch Mode: states[file_path][model_name]
            files.forEach(f => {
                this.states[f] = {};
                models.forEach(m => {
                    this.states[f][m] = this._getInitialState(m);
                });
            });
            this.dashboardLines = files.length * models.length + 3;
        } else {
            // Dialog Mode: states[model_name]
            models.forEach(m => {
                this.states[m] = this._getInitialState(m);
            });
            this.dashboardLines = models.length + 3;
        }
    }

    _getInitialState(model) {
        return {
            progress: 0, 
            entropy: Math.random() * 100, 
            influence: Math.random() * 0.4 + 0.6, 
            output: '',
            model: model
        };
    }

    _clearLine() { readline.cursorTo(process.stdout, 0); readline.clearLine(process.stdout, 0); }

    /** Updates state with strict entropy resolution. */
    updateState(key1, key2 = null) {
        let s = key2 ? this.states[key1][key2] : this.states[key1];
        
        s.progress = Math.min(1, s.progress + 0.05 + Math.random() * 0.02);
        
        // Strict Entropy Resolution: Higher Influence rapidly reduces Entropy.
        const entropyDecay = (s.influence * 15) * Math.random(); 
        s.entropy = Math.max(0, s.entropy - entropyDecay);
        
        s.influence = Math.min(1, s.influence + (Math.random() - 0.4) * 0.01); 
    }
    
    drawDashboard(status = 'RUNNING') {
        readline.cursorTo(process.stdout, 0);
        // Move cursor up to overwrite previous dashboard
        readline.moveCursor(process.stdout, 0, -this.dashboardLines); 
        
        const statusColor = status === 'CONVERGED' ? '\x1b[42m' : '\x1b[44m';
        const iterDisplay = this.files.length ? `| Iteration: ${this.currentIter}/${this.maxIter}` : '';
        const mode = this.files.length ? 'BATCH CONSENSUS' : 'DIALOG FUSION';

        let dashboardOutput = `${statusColor}| LIVE MODEL DASHBOARD | Mode: ${mode}${iterDisplay} | STATUS: ${status}\x1b[0m\n`;
        dashboardOutput += `|--------------------------------------------------------------------------------\n`;

        const renderItem = (key1, key2 = null) => {
            const s = key2 ? this.states[key1][key2] : this.states[key1];
            const name = key2 ? `${path.basename(key1)}|${s.model}` : s.model;
            const color = getLabelColor(name);
            const baseName = name.padEnd(20);
            
            const perc = (s.progress * 100).toFixed(0).padStart(3);
            const entropyBar = '█'.repeat(Math.floor(s.entropy / 10)).padEnd(10);
            const influenceBar = '▒'.repeat(Math.floor(s.influence * 10)).padEnd(10);

            dashboardOutput += `| ${color}${baseName}\x1b[0m | PROG: ${perc}% | ENT: [\x1b[33m${entropyBar}\x1b[0m] | INF: [\x1b[35m${influenceBar}\x1b[0m]\n`;
        };

        if (this.files.length) {
            for (const f of this.files) { for (const m of this.models) { renderItem(f, m); } }
        } else {
            for (const m of this.models) { renderItem(m); }
        }
        
        dashboardOutput += `|--------------------------------------------------------------------------------`;

        const outputLines = dashboardOutput.split('\n');
        for(let i = 0; i < this.dashboardLines; i++) {
            this._clearLine(); 
            if (i < outputLines.length) {
                process.stdout.write(outputLines[i]);
            }
            if (i < this.dashboardLines - 1) {
                process.stdout.write('\n');
            }
        }
        
        readline.moveCursor(process.stdout, 0, 1);
        readline.cursorTo(process.stdout, 0); 
    }

    /** Helper to simulate one chunk of streaming response from a single model. */
    async _simulateModelChunk(model, prompt) {
        const token = `TKN_${model}_${Math.random().toFixed(4)} `;
        await new Promise(r => setTimeout(r, 10 + Math.random() * 20));
        return token;
    }
    
    /** Helper to simulate a single token chunk for batch mode */
    async _simulateBatchToken(file, model, prompt) {
        // Simulate complex token based on file and model context
        const token = `TKN(${path.basename(file)}|${model})_${Math.random().toFixed(4)} `;
        await new Promise(r => setTimeout(r, 5 + Math.random() * 15));
        return token;
    }

    /**
     * Runs interactive dialog with parallel streaming and token-by-token fusion.
     * @param {string} prompt The user input.
     */
    async runDialog(prompt) {
        const modelResponses = {};
        
        // Initial dashboard draw to reserve space
        for(let i = 0; i < this.dashboardLines; i++) process.stdout.write('\n');
        this.drawDashboard('INITIALIZING');

        process.stdout.write(`\n\x1b[1;37m--- DIALOG FUSION STARTED ---\x1b[0m\n`);

        const MAX_DIALOG_TOKENS = 50;

        for (let tokenIter = 0; tokenIter < MAX_DIALOG_TOKENS; tokenIter++) { 
            const modelPromises = this.models.map(async (model) => {
                const chunk = await this._simulateModelChunk(model, prompt);
                this.updateState(model);
                this.states[model].output += chunk; 
                modelResponses[model] = chunk;
            });

            await Promise.all(modelPromises); 

            // --- SIMULTANEOUS TOKEN STREAM (Verbose Debug View) ---
            let individualStreamLine = '';
            for (const model of this.models) {
                 const color = getLabelColor(model);
                 const chunk = modelResponses[model].trim();
                 individualStreamLine += `${color}[${model}]\x1b[0m > ${chunk.substring(0, 10).padEnd(10)} | `;
            }
            this._clearLine();
            process.stdout.write(`\x1b[90m${individualStreamLine}\x1b[0m\n`);

            // --- FUSION LOGIC ---
            let maxInfluence = -1;
            let influencerModel = '';
            for (const model of this.models) {
                const s = this.states[model];
                if (s.influence > maxInfluence) {
                    maxInfluence = s.influence;
                    influencerModel = model;
                }
            }
            let fusedToken = influencerModel ? modelResponses[influencerModel] || '' : '';
            
            // Stream Fused Meta-Response
            process.stdout.write(`\x1b[1;36m[META-FUSION|INF:${maxInfluence.toFixed(2)}]\x1b[0m > ${fusedToken.trim()} `);
            this.fusedOutput += fusedToken;
            
            this.drawDashboard('FUSING');
            
            if (tokenIter > 30 && Math.random() < 0.3) break;
        }

        this.drawDashboard('COMPLETE');
        process.stdout.write('\n\x1b[1;32m[DIALOG COMPLETE]\x1b[0m Fusion stream ended.\n');
    }

    /**
     * Runs batch processing with token-level streaming and real-time fusion race visualization.
     */
    async runBatch() { 
        const MAX_STREAM_STEPS = 40; 
        
        for (let iter = 1; iter <= this.maxIter; iter++) {
            this.currentIter = iter;
            process.stdout.write(`\n\x1b[1;37m--- BATCH ITERATION ${iter} STREAMING RACE ---\x1b[0m\n`); 
            
            let tokenPosition = 0;

            // Reset outputs for new iteration
            for (const f of this.files) {
                for (const m of this.models) {
                    this.states[f][m].output = '';
                }
            }

            // --- 1. Token Race Loop (Visualization) ---
            while (tokenPosition < MAX_STREAM_STEPS) {
                tokenPosition++;
                
                const chunkPromises = [];
                
                // Run a parallel task for every file/model to generate one token chunk
                for (const f of this.files) {
                    for (const m of this.models) {
                        // Ensure we only generate new tokens if the current output length is less than the current token position
                        if (this.states[f][m].output.trim().split(/\s+/).length >= tokenPosition) continue;

                        chunkPromises.push((async () => {
                            const chunk = await this._simulateBatchToken(f, m, this.urlContent);
                            this.updateState(f, m); // Update metrics
                            this.states[f][m].output += chunk;
                        })());
                    }
                }

                if (chunkPromises.length === 0 && tokenPosition > 1) {
                    break; 
                }

                await Promise.all(chunkPromises);
                this.drawDashboard('STREAMING');

                // --- 2. Live Fusion (Token-by-Token across ALL file/model results) ---
                
                let liveFusionResult = '';

                for (const f of this.files) {
                    let maxInfluence = -1;
                    let chosenToken = '';
                    
                    for (const m of this.models) {
                        const s = this.states[f][m];
                        const tokens = s.output.trim().split(/\s+/);
                        
                        // Check if a token exists at the current step position
                        if (tokens.length > tokenPosition - 1) {
                            if (s.influence > maxInfluence) {
                                maxInfluence = s.influence;
                                chosenToken = tokens[tokenPosition - 1]; // Select the token chunk generated in this step
                            }
                        }
                    }
                    
                    // Print the fused token for this file's consensus point
                    if (chosenToken) {
                        const color = getLabelColor(f);
                        liveFusionResult += `${color}[FUSED:${path.basename(f)}|INF:${maxInfluence.toFixed(2)}]\x1b[0m ${chosenToken.trim()} `;
                    }
                }

                // Print the live stream line
                this._clearLine();
                process.stdout.write(`\x1b[1;36m[LIVE BATCH STREAM]\x1b[0m > ${liveFusionResult}\n`); 
                await new Promise(r => setTimeout(r, 20));
            } // End streaming loop

            // 3. Post-Stream Convergence Check (Final Token Fusion for Signature)
            
            let finalFusedTokens = [];
            for (const f of this.files) { 
                const modelTokens = this.models.map(m => this.states[f][m].output.trim().split(/\s+/));
                const maxTokens = Math.max(...modelTokens.map(t => t.length));
                
                for (let i = 0; i < maxTokens; i++) {
                    let maxInfluence = -1;
                    let chosenToken = '';
                    
                    for (let j = 0; j < this.models.length; j++) {
                        const m = this.models[j];
                        const s = this.states[f][m];
                        const tokens = modelTokens[j];
                        
                        if (tokens[i] && s.influence > maxInfluence) {
                            maxInfluence = s.influence;
                            chosenToken = tokens[i];
                        }
                    }
                    if (chosenToken) finalFusedTokens.push(chosenToken);
                }
            }
            
            this.fusedOutput = finalFusedTokens.join(' ');
            const sig = this.getSignature(this.fusedOutput);

            if (this.lastSignature && sig === this.lastSignature) {
                this.drawDashboard('CONVERGED');
                console.log(`\n\x1b[32m[CONVERGED] at iteration ${iter}\x1b[0m\n`);
                break;
            }
            
            this.lastSignature = sig;
            process.stdout.write(`\n\x1b[1;32m[ITER ${iter} FUSED SIG:${sig.substring(0,10)}...]\x1b[0m\n`);
            this.drawDashboard('RUNNING');
        } 
        this.drawDashboard('COMPLETE');
        console.log(`\x1b[1;36m[Batch Meta-Consensus Complete]\x1b[0m`);
    }

    getSignature(output) { return output.substring(0, 100); }
}

// --- CLI UTILITIES AND ENTRY POINT ---

function printUsage() {
    console.log(`\n\x1b[1;37mUsage: node ai_meta_cli.js <command> [flags]\x1b[0m`);
    console.log(`\nCommands:`);
    console.log(`  \x1b[1;36mdialog\x1b[0m [flags]     - Starts a multi-model, fused, streaming REPL session.`);
    console.log(`  \x1b[1;36mbatch\x1b[0m   [flags]     - Runs concurrent, converging meta-consensus on files.`);
    
    console.log(`\n\x1b[1;37mFlags:\x1b[0m`);
    console.log(`  \x1b[35m--models=<name1,name2>\x1b[0m (Both) Comma-separated list of models (default: core).`);
    console.log(`  \x1b[35m--files=<glob>\x1b[0m         (Batch) Glob pattern to load files (simulated).`);
    console.log(`  \x1b[35m--iterations=<N>\x1b[0m     (Batch) Max convergence iterations (default: 8).`);
    
    console.log(`\nExample:`);
    console.log(`  node ai_meta_cli.js dialog --models=loop,code,coin`);
    console.log(`  node ai_meta_cli.js batch --files="*.py" --models=core,2244\n`);
}

/** SIMULATION of fetch URL content dynamically. */
async function fetchURLContent(url) {
    if (url.startsWith('http')) {
        console.log(`\n\x1b[1;32mhttps://fetch.com/\x1b[0m \x1b[90mSimulating fetch from: ${url}\x1b[0m`);
    }
    await new Promise(r => setTimeout(r, 200)); 
    return MOCK_URL_CONTENT;
}

/** SIMULATION of loading file paths from a glob pattern. */
function loadFilesFromGlob(pattern) {
    const allFiles = Object.keys(MOCK_FILE_CONTENTS);
    
    const files = allFiles.filter(p => new RegExp(pattern.replace(/\./g, '\\.').replace(/\*/g, '.*')).test(p));
    
    if (files.length === 0) {
        console.warn(`\x1b[33m[WARN]\x1b[0m No files matched pattern '${pattern}'. Using default mock files.`);
        return allFiles; 
    }
    return files;
}

async function main() {
    const rawArgs = process.argv.slice(2);
    const command = rawArgs[0];
    const flagArgs = rawArgs.slice(1);
    const flags = parseFlags(rawArgs); // Use rawArgs to ensure command is included in flags check
    
    if (!command) {
        printUsage();
        return;
    }

    const backend = flags.backend || 'ollama';
    const apiKey = process.env.OPENAI_API_KEY || '';
    const models = flags.models ? flags.models.split(',').map(m => m.trim()) : ['core'];

    if (command === 'dialog') {
        const engine = new UnifiedMetaEngine({ models, backend, apiKey });
        
        const rl = readline.createInterface({ 
            input: process.stdin, 
            output: process.stdout, 
            prompt: '\x1b[1;33mYou >\x1b[0m ' 
        });

        rl.prompt();

        rl.on('line', async (line) => {
            const prompt = line.trim();
            if (prompt.toLowerCase() === 'exit') { rl.close(); return; }
            if (!prompt) { rl.prompt(); return; }
            
            try {
                await engine.runDialog(prompt);
            } catch (err) {
                console.error(`\n\x1b[31m[ERROR]\x1b[0m Fusion failure: ${err.message}`);
            }
            rl.prompt();
        });

        rl.on('close', () => {
            console.log('\n\x1b[1;36m[CLI DIALOG]\x1b[0m Session ended.');
            process.exit(0);
        });

    } else if (command === 'batch') {
        const filePattern = flags.files || './src/*';
        const urlArg = flags.url || 'http://default.mock.url';
        const maxIter = flags.iterations ? parseInt(flags.iterations) : 8;

        if (isNaN(maxIter) || maxIter <= 0) {
             console.error(`\x1b[31m[ERROR]\x1b[0m --iterations must be a positive integer.`);
             return;
        }

        const filePaths = loadFilesFromGlob(filePattern);
        const urlContent = await fetchURLContent(urlArg);

        if (filePaths.length === 0) {
            console.error(`\x1b[31m[ERROR]\x1b[0m No files found for processing.`);
            return;
        }

        const engine = new UnifiedMetaEngine({
            files: filePaths, 
            models: models, 
            backend: backend, 
            apiKey: apiKey, 
            maxIter: maxIter,
            urlContent: urlContent
        });
        
        await engine.runBatch();
    } else {
        console.error(`\x1b[31m[ERROR]\x1b[0m Unknown command: '${command}'`);
        printUsage();
    }
}

main();
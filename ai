#!/usr/bin/env bash
# ~/.bin/ai - Bulletproof AI CLI for Termux + Proot + Local AI
# Requires: python3-full, git, sqlite3, nodejs, curl, wget

set -euo pipefail
IFS=$'\n\t'

AI_HOME="$HOME/.local_ai"
DB="$AI_HOME/core.db"
MODULES_DIR="$AI_HOME/modules"
LOGS="$HOME/logs"

mkdir -p "$AI_HOME" "$MODULES_DIR" "$LOGS"

log() { echo "[AI $(date '+%H:%M:%S')] $*"; }

# -------------------------
# Ensure DB tables exist
# -------------------------
sqlite3 "$DB" <<SQL
CREATE TABLE IF NOT EXISTS mindflow(
    id INTEGER PRIMARY KEY,
    session_id TEXT,
    loop_id INTEGER,
    model_name TEXT,
    output TEXT,
    rank INTEGER,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);
CREATE TABLE IF NOT EXISTS task_logs(
    id INTEGER PRIMARY KEY,
    tool_used TEXT,
    args TEXT,
    output_summary TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);
CREATE TABLE IF NOT EXISTS cache(
    prompt_hash TEXT PRIMARY KEY,
    final_answer TEXT
);
SQL

# -------------------------
# Load local modules
# -------------------------
MODULES=("blockchain" "nostr" "lightning" "termux" "proot" "url-parser" "snippet-assembler")
for m in "${MODULES[@]}"; do
    MODULE_FILE="$MODULES_DIR/$m.sh"
    if [ -f "$MODULE_FILE" ]; then
        source "$MODULE_FILE"
    else
        log "âš  Module $m not found, creating placeholder"
        echo "# placeholder for $m" > "$MODULE_FILE"
    fi
done

# -------------------------
# Start Ollama if not running
# -------------------------
if ! pgrep -x ollama >/dev/null 2>&1; then
    log "Starting Ollama server..."
    nohup ollama serve > "$LOGS/ollama_server.log" 2>&1 &
    sleep 2
fi

# -------------------------
# Preprocess prompt
# -------------------------
if [ $# -eq 0 ]; then
    log "Usage: ai 'your prompt here'"
    exit 1
fi

PROMPT="$*"

# Translate Chinese characters to English placeholder
PROMPT=$(echo "$PROMPT" | sed 's/[\p{Han}]/[TRANSLATE TO EN]/g')

# -------------------------
# Check cache first
# -------------------------
PROMPT_HASH=$(echo -n "$PROMPT" | sha256sum | awk '{print $1}')
CACHE=$(sqlite3 "$DB" "SELECT final_answer FROM cache WHERE prompt_hash='$PROMPT_HASH';")
if [ -n "$CACHE" ]; then
    echo "$CACHE"
    exit 0
fi

# -------------------------
# Query Ollama safely
# -------------------------
OUTPUT=""
PYCODE=$(cat <<'EOF'
import subprocess, json, sys
prompt = sys.argv[1]
try:
    result = subprocess.run(
        ["ollama", "run", "2244:latest", "--json", prompt],
        capture_output=True, text=True, check=True
    )
    data = json.loads(result.stdout)
    print(data.get("answer", ""))
except subprocess.CalledProcessError:
    print("[ERROR] Ollama query failed")
except json.JSONDecodeError:
    print("[ERROR] Invalid JSON from Ollama")
EOF
)

OUTPUT=$(python3 -c "$PYCODE" "$PROMPT")

# -------------------------
# Save to cache
# -------------------------
sqlite3 "$DB" "INSERT OR REPLACE INTO cache(prompt_hash, final_answer) VALUES('$PROMPT_HASH','$OUTPUT');"

# -------------------------
# Display output
# -------------------------
echo "$OUTPUT"
#!/usr/bin/env bash
# ~/.bin/ai - Bulletproof AI CLI for Termux/Proot
set -euo pipefail
IFS=$'\n\t'

AI_HOME="${HOME}/.local_ai"
MODEL="2244:latest"
OLLAMA_API="http://127.0.0.1:11434"

# Logging
log(){ echo "[$(date '+%H:%M:%S')] $*"; }

# Load environment safely
[ -f "$HOME/.env.local" ] && set -a && source "$HOME/.env.local" && set +a

# Check Ollama server
if ! curl -s -f -o /dev/null "$OLLAMA_API/"; then
    log "[ERROR] Ollama server not running at $OLLAMA_API"
    exit 1
fi

# Check model exists
if ! ollama list | grep -q "$MODEL"; then
    log "[ERROR] Model $MODEL not installed. Run 'ollama pull $MODEL'"
    exit 1
fi

# Query function
query_ai() {
    local prompt="$1"

    # Ensure only English/German output
    local wrapped_prompt="Respond only in English or German. Translate any non-English/German input. Prompt: $prompt"

    # Call Ollama run, strip non-JSON lines
    local response
    if ! response=$(ollama run "$MODEL" --json "$wrapped_prompt" 2>/dev/null); then
        log "[ERROR] Ollama query failed"
        return 1
    fi

    # Validate JSON
    if ! echo "$response" | python3 -c "import sys, json; json.load(sys.stdin)" >/dev/null 2>&1; then
        log "[ERROR] Invalid JSON response from Ollama"
        return 1
    fi

    # Extract 'answer' safely
    echo "$response" | python3 -c "
import sys, json
r=json.load(sys.stdin)
ans = r.get('answer','')
# Translate Chinese to English automatically
if any('\u4e00' <= c <= '\u9fff' for c in ans):
    ans = 'âš  Original contained Chinese: ' + ans
print(ans)
"
}

# Main
if [ $# -eq 0 ]; then
    log "Usage: ai 'your prompt'"
    exit 1
fi

query_ai "$*"

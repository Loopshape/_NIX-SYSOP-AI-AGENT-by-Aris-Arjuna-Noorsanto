#!/usr/bin/env bash
# ~/.bin/ai - Execute DB-stored modules & query Ollama safely
# - Modules stored in core.db.modules (name, code)
# - Executes modules from DB in an ephemeral sandbox
# - Uses Ollama CLI for prompts (tries --json then fallback)
# - Caches responses as BLOBs in DB
# - Syntax-highlights code blocks (pygments if available)
# - Tries to auto-translate Chinese if `trans` exists; otherwise marks it
#
# Requirements: python3, sqlite3, ollama CLI
# Optional: pygments (pip install pygments), translate-cli (trans), timeout

set -euo pipefail
IFS=$'\n\t'

AI_HOME="${HOME}/.local_ai"
DB="$AI_HOME/core.db"
LOG_DIR="$AI_HOME/logs"
TMP_DIR="${AI_HOME}/tmp"
MODEL="${AI_MODEL:-2244:latest}"
TIMEOUT_CMD="${TIMEOUT_CMD:-timeout}"   # if GNU timeout is present, otherwise will run without timeout
MODULE_EXEC_TIMEOUT=${MODULE_EXEC_TIMEOUT:-120}   # seconds
MEMORY_LIMIT_KB=${MEMORY_LIMIT_KB:-2097152}      # ~2GB virtual memory limit for ulimit -v

mkdir -p "$AI_HOME" "$LOG_DIR" "$TMP_DIR"

log(){ printf "[%s] %s\n" "$(date '+%H:%M:%S')" "$*"; }

# --- Ensure DB and tables exist (modules, mindflow, task_logs, cache) ---
python3 - <<PY
import sqlite3, os
db = os.path.expanduser("$DB")
os.makedirs(os.path.dirname(db), exist_ok=True)
conn = sqlite3.connect(db)
cur = conn.cursor()
cur.execute('''CREATE TABLE IF NOT EXISTS modules(
    name TEXT PRIMARY KEY,
    code BLOB,
    last_update DATETIME DEFAULT CURRENT_TIMESTAMP
);''')
cur.execute('''CREATE TABLE IF NOT EXISTS mindflow(
    id INTEGER PRIMARY KEY, session_id TEXT, loop_id INTEGER,
    model_name TEXT, output BLOB, rank INTEGER,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);''')
cur.execute('''CREATE TABLE IF NOT EXISTS task_logs(
    id INTEGER PRIMARY KEY, tool_used TEXT, args BLOB, output_summary BLOB,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);''')
cur.execute('''CREATE TABLE IF NOT EXISTS cache(
    prompt_hash TEXT PRIMARY KEY, prompt BLOB, final_answer BLOB
);''')
conn.commit()
conn.close()
PY

# ---------- Helpers ----------

# highlight output (supports multi-block fenced code) via Python/pygments if installed
highlight_output() {
  local text="$1"
  python3 - <<PY
import sys, re
text = """$text"""
# escape triple-quotes issues by reading from stdin if content is huge/newlines
# but here we pass via heredoc; for safety, use raw text variable.
try:
    from pygments import highlight
    from pygments.lexers import guess_lexer, TextLexer
    from pygments.formatters import TerminalFormatter
    parts = re.split(r'```(.*?)```', text, flags=re.DOTALL)
    out = ""
    for i, p in enumerate(parts):
        if i % 2 == 1:
            code = p
            try:
                lexer = guess_lexer(code)
            except Exception:
                lexer = TextLexer()
            out += highlight(code, lexer, TerminalFormatter())
        else:
            out += p
    sys.stdout.write(out)
except Exception:
    # fallback: print raw
    sys.stdout.write(text)
PY
}

# safe run of ollama: try --json then fallback to plain output
run_ollama_safe() {
  local prompt="$1"
  local model="$2"
  python3 - <<PY
import subprocess, json,sys
prompt = sys.argv[1]
model = sys.argv[2]
def try_json():
    try:
        r = subprocess.run(["ollama","run",model,"--json",prompt], capture_output=True, text=True, check=True)
        return json.loads(r.stdout).get("answer","")
    except Exception:
        return None
def try_plain():
    try:
        r = subprocess.run(["ollama","run",model,prompt], capture_output=True, text=True, check=True)
        return r.stdout.strip()
    except Exception as e:
        print(f"[ERROR] Ollama run failed: {e}", file=sys.stderr)
        return None
out = try_json()
if out is None:
    out = try_plain()
if out is None:
    sys.exit(2)
# filter to English/German lines only (keep lines that contain A-Za-z or German-specific chars)
import re
lines = []
for l in out.splitlines():
    if re.search(r'[A-Za-zäöüßÄÖÜ]', l):
        lines.append(l)
sys.stdout.write("\n".join(lines))
PY
}

# store cache and optionally mindflow/task_logs entries via python
db_store_cache() {
  local prompt="$1"
  local answer="$2"
  python3 - <<PY
import sqlite3, os, sys, hashlib
db = os.path.expanduser("$DB")
prompt = sys.argv[1].encode('utf-8')
answer = sys.argv[2].encode('utf-8')
h = hashlib.sha256(prompt).hexdigest()
conn = sqlite3.connect(db)
cur = conn.cursor()
cur.execute("INSERT OR REPLACE INTO cache(prompt_hash,prompt,final_answer) VALUES(?,?,?)",(h,prompt,answer))
conn.commit()
conn.close()
PY
}

db_log_tool() {
  local tool="$1"
  local args="$2"
  local result="$3"
  python3 - <<PY
import sqlite3, os, sys
db = os.path.expanduser("$DB")
conn = sqlite3.connect(db)
cur = conn.cursor()
cur.execute("INSERT INTO task_logs(tool_used,args,output_summary) VALUES(?,?,?)", (sys.argv[1].encode('utf-8'), sys.argv[2].encode('utf-8'), sys.argv[3].encode('utf-8')))
conn.commit()
conn.close()
PY
}

# fetch module code from DB (returns empty if not found)
get_module_code() {
  local name="$1"
  python3 - <<PY
import sqlite3, os, sys
db = os.path.expanduser("$DB")
conn = sqlite3.connect(db)
cur = conn.cursor()
cur.execute("SELECT code FROM modules WHERE name=?", (sys.argv[1],))
r = cur.fetchone()
conn.close()
if not r:
    sys.exit(1)
code = r[0]
# ensure we print raw code
sys.stdout.write(code)
PY
}

# ---------- Module execution ----------
# Usage: execute_module_from_db <module_name> [arg1 arg2 ...]
execute_module_from_db() {
  local module="$1"; shift
  local args=("$@")
  log "Executing module from DB: $module (args: ${args[*]})"

  # get code
  local code
  if ! code=$(get_module_code "$module"); then
    log "ERROR: module '$module' not found in DB"
    return 2
  fi

  # create isolated temp dir (prefer tmpfs if mounted)
  local run_dir
  run_dir="$(mktemp -d "$TMP_DIR/ai_mod.XXXX")"
  chmod 700 "$run_dir"
  local script_path="$run_dir/$module.sh"

  # write code safely
  printf '%s' "$code" > "$script_path"
  chmod 700 "$script_path"

  # prepare environment (limited)
  # create minimal sandbox env
  local oldwd
  oldwd="$PWD"
  cd "$run_dir"

  # set resource limits: CPU time and virtual memory if possible
  # Use ulimit inside a subshell for module execution
  local exec_output exec_status
  if command -v "$TIMEOUT_CMD" >/dev/null 2>&1; then
    # Use timeout if available
    exec_output=$( (ulimit -t 120 -v $MEMORY_LIMIT_KB 2>/dev/null || true; bash "$script_path" "${args[@]}" ) 2>&1 | $TIMEOUT_CMD -s KILL $MODULE_EXEC_TIMEOUT cat ) || exec_status=$?
  else
    exec_output=$( (ulimit -t 120 -v $MEMORY_LIMIT_KB 2>/dev/null || true; bash "$script_path" "${args[@]}" ) 2>&1 ) || exec_status=$?
  fi

  # restore
  cd "$oldwd"
  # cleanup temp dir
  # keep script for debugging but remove later
  rm -rf "$run_dir"

  # log into DB task_logs
  db_log_tool "$module" "${args[*]:-}" "$exec_output"

  # Try to highlight output (codeblocks) and print
  highlight_output "$exec_output"

  return ${exec_status:-0}
}

# ---------- Prompt handling ----------
# two modes:
#  - module execution: first argument "module:<name>" or subcommand "run-module <name> [args]"
#  - normal AI prompt: send to ollama

if [ $# -lt 1 ]; then
  echo "Usage: ai module:<name> [args...]   OR   ai 'your question/prompt'"
  exit 1
fi

first="$1"
shift || true

if [[ "$first" == module:* ]]; then
  modname="${first#module:}"
  execute_module_from_db "$modname" "$@"
  exit $?
fi

if [[ "$first" == "run-module" ]]; then
  if [ $# -lt 1 ]; then
    echo "Usage: ai run-module <name> [args...]"
    exit 1
  fi
  modname="$1"; shift
  execute_module_from_db "$modname" "$@"
  exit $?
fi

# default: treat as prompt to AI
PROMPT="$first"
# if more args, append them
if [ $# -gt 0 ]; then
  PROMPT="$PROMPT $*"
fi

# try cache first
PROMPT_HASH=$(python3 - <<PY
import hashlib, sys
print(hashlib.sha256(sys.argv[1].encode('utf-8')).hexdigest())
PY
"$PROMPT")

cached=$(python3 - <<PY
import sqlite3, os, sys
db = os.path.expanduser("$DB")
conn = sqlite3.connect(db)
cur = conn.cursor()
cur.execute("SELECT final_answer FROM cache WHERE prompt_hash=?", (sys.argv[1],))
r = cur.fetchone()
conn.close()
if r:
    sys.stdout.write(r[0] if isinstance(r[0], str) else r[0].decode('utf-8'))
PY
"$PROMPT_HASH")

if [ -n "$cached" ]; then
  log "Cache hit — outputting cached answer"
  highlight_output "$cached"
  exit 0
fi

log "Query Ollama model: $MODEL"
ollama_out="$(run_ollama_safe "$PROMPT" "$MODEL" 2>&1 || true)"

if [ -z "$ollama_out" ]; then
  log "[ERROR] Ollama returned empty or failed. See $LOG_DIR/ollama.log"
  echo "[ERROR] Ollama query failed"
  exit 1
fi

# if Chinese detected and translator exists, try to translate
if echo "$ollama_out" | grep -P '[\x{4e00}-\x{9fff}]' >/dev/null 2>&1; then
  if command -v trans >/dev/null 2>&1; then
    log "Translating Chinese -> English using trans"
    translated=$(printf '%s' "$ollama_out" | trans -b :en 2>/dev/null || echo "")
    if [ -n "$translated" ]; then
      ollama_out="$ollama_out\n\n[Translated->en]: $translated"
    else
      ollama_out="$ollama_out\n\n[Translated->en not available]"
    fi
  else
    ollama_out="$ollama_out\n\n[Detected Chinese: translation not available]"
  fi
fi

# store to cache
db_store_cache "$PROMPT" "$ollama_out"

# syntax-highlight and output
highlight_output "$ollama_out"

exit 0
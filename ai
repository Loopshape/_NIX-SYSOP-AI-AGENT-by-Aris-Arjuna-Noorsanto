#!/usr/bin/env bash
# Strict AI CLI - English/German only, local modules, Ollama query
set -euo pipefail
IFS=$'\n\t'

# === Paths ===
AI_HOME="$HOME/.local_ai"
MODULES="$AI_HOME/modules"
DB="$AI_HOME/core.db"
SANDBOX="$AI_HOME/sandbox"
mkdir -p "$MODULES" "$SANDBOX" "$HOME/logs"

log(){ echo "[$(date '+%H:%M:%S')] $*"; }

# === Load Environment ===
[ -f "$HOME/.env.local" ] && source "$HOME/.env.local"

# === Verify Python3 ===
command -v python3 >/dev/null || { log "âš  python3 not found"; exit 1; }

# === Initialize SQLite DB ===
if [ ! -f "$DB" ]; then
    log "ðŸ“¦ Initializing AI core database..."
    sqlite3 "$DB" "CREATE TABLE IF NOT EXISTS mindflow(id INTEGER PRIMARY KEY, session_id TEXT, loop_id INTEGER, model_name TEXT, output TEXT, rank INTEGER, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP);"
    sqlite3 "$DB" "CREATE TABLE IF NOT EXISTS task_logs(id INTEGER PRIMARY KEY, tool_used TEXT, args TEXT, output_summary TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP);"
    sqlite3 "$DB" "CREATE TABLE IF NOT EXISTS cache(prompt_hash TEXT PRIMARY KEY, final_answer TEXT);"
fi

# === Load local modules only ===
MODULE_LIST=("blockchain" "nostr" "lightning" "termux" "proot" "url-parser" "snippet-assembler")
for mod in "${MODULE_LIST[@]}"; do
    MOD_FILE="$MODULES/$mod.sh"
    [ -f "$MOD_FILE" ] || {
        log "âš  Module $mod missing, creating placeholder..."
        echo "#!/usr/bin/env bash
log(){ echo \"[\$(date '+%H:%M:%S')] [MODULE $mod] \$*\"; }" > "$MOD_FILE"
        chmod +x "$MOD_FILE"
    }
done

# === Start Ollama server if not running ===
if ! pgrep -x ollama >/dev/null 2>&1; then
    log "ðŸš€ Starting Ollama server..."
    nohup ollama serve > "$HOME/logs/ollama_server.log" 2>&1 &
    sleep 3
fi

# === AI Prompt ===
PROMPT="${1:-Hello AI}"
log "ðŸ§  Querying AI (English/German only): $PROMPT"

# === Python code for Ollama query + cache ===
python3 - <<'EOF' "$PROMPT"
import sys, json, hashlib, subprocess, sqlite3, os

prompt = sys.argv[1]
db = os.path.expanduser("~/.local_ai/core.db")

# Hash for caching
prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()
conn = sqlite3.connect(db)
cur = conn.cursor()
cur.execute("SELECT final_answer FROM cache WHERE prompt_hash=?", (prompt_hash,))
row = cur.fetchone()
if row:
    print(row[0])
    sys.exit(0)

# Call Ollama model safely
try:
    result = subprocess.run(
        ["ollama", "run", "2244:latest", "", prompt],
        capture_output=True, text=True, check=True
    )
    data = json.loads(result.stdout)
    answer = data.get("answer","").strip()
except Exception as e:
    answer = f"[ERROR] Ollama query failed: {e}"

# Force English/German only
if any(ord(c) > 255 for c in answer):
    answer = "[INFO] Non-English/German characters removed."
    
# Store in cache
cur.execute("INSERT OR REPLACE INTO cache(prompt_hash, final_answer) VALUES (?,?)", (prompt_hash, answer))
conn.commit()
conn.close()

print(answer)
EOF
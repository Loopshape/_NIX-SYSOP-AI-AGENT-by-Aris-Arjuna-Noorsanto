#!/bin/bash

# ai - Autonomous Intelligence Platform
# Unified orchestrator with multi-model reasoning, FPI scoring, and memory systems
# Version 2.1.0 - Complete Python implementation in single shell script

set -euo pipefail

# --- Configuration ---
VERSION="2.1.0"
AUTHOR="Autonomic Synthesis Platform"
PYTHON_APP_DIR="${HOME}/.ai_platform_app"
MAIN_PY_FILE="${PYTHON_APP_DIR}/ai_platform.py"
VENV_DIR="${PYTHON_APP_DIR}/venv"
REQUIREMENTS_FILE="${PYTHON_APP_DIR}/requirements.txt"

# Colors for output
ANSI_BOLD="\033[1m"
ANSI_GREEN="\033[32m"
ANSI_CYAN="\033[36m"
ANSI_YELLOW="\033[33m"
ANSI_RED="\033[31m"
ANSI_GRAY="\033[90m"
ANSI_RESET="\033[0m"

# --- Helper Functions ---
print_info() {
    echo -e "${ANSI_CYAN}ℹ️  $1${ANSI_RESET}"
}

print_success() {
    echo -e "${ANSI_GREEN}✅ $1${ANSI_RESET}"
}

print_warning() {
    echo -e "${ANSI_YELLOW}⚠️  $1${ANSI_RESET}"
}

print_error() {
    echo -e "${ANSI_RED}❌ $1${ANSI_RESET}" >&2
}

print_header() {
    echo -e "\n${ANSI_CYAN}${ANSI_BOLD}=== $1 ===${ANSI_RESET}"
}

# --- Python Application Creation ---
create_python_app() {
    print_header "CREATING AI PLATFORM APPLICATION"

    mkdir -p "$PYTHON_APP_DIR"

    cat > "$MAIN_PY_FILE" << 'PYTHON_EOF'
#!/usr/bin/env python3
"""
Autonomic Intelligence Platform v2.1.0
Unified orchestrator with multi-model reasoning, FPI scoring, and memory systems
"""

import os
import sys
import asyncio
import aiohttp
import sqlite3
import json
import hashlib
import gzip
import argparse
import random
import logging
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from contextlib import contextmanager

# --- Configuration ---
@dataclass
class DatabaseConfig:
    path: str
    swap_dir: str
    log_file: str

@dataclass
class ModelConfig:
    name: str
    description: str
    system_prompt: str
    weight: int
    fallback_template: str

@dataclass
class OrchestratorConfig:
    max_loops: int = 5
    loop_extension_threshold: int = 100
    compression_threshold: int = 1000
    cleanup_days: int = 30
    enable_fpi_scoring: bool = True

@dataclass
class OllamaConfig:
    base_url: str = "http://localhost:11434"
    default_model: str = "llama2"
    timeout: int = 30
    fallback_enabled: bool = True

class AppConfig:
    VERSION = "2.1.0"
    AUTHOR = "Autonomic Synthesis Platform"

    def __init__(self):
        self.home_dir = Path.home() / ".ai_platform"
        self.home_dir.mkdir(exist_ok=True)

        self.database = DatabaseConfig(
            path=str(self.home_dir / "core.db"),
            swap_dir=str(self.home_dir / "swap"),
            log_file=str(self.home_dir / "ai.log")
        )

        self.ollama = OllamaConfig()
        self.orchestrator = OrchestratorConfig()

        self.models: Dict[str, ModelConfig] = {
            "code": ModelConfig(
                "code",
                "technical reasoning, programming, algorithms",
                "You are an expert technical reasoning AI. Provide detailed, logical analysis with code examples when appropriate. Focus on algorithms, data structures, and technical implementation.",
                1000,
                "// Technical analysis unavailable. Fallback: Consider implementing solution using modular architecture with error handling."
            ),
            "coin": ModelConfig(
                "coin",
                "mood, context, historical analysis, emotional intelligence",
                "You are a context-aware AI that understands mood, time, and historical context. Respond with emotional intelligence and contextual awareness.",
                800,
                "Contextual analysis: Current systems offline. In such moments, reflection often reveals alternative perspectives worth exploring."
            ),
            "2244": ModelConfig(
                "2244",
                "language prioritization, German/English, cultural context",
                "You are a multilingual AI that prioritizes language appropriateness. Switch between German and English as needed for optimal communication.",
                800,
                "Sprachanalyse derzeit nicht verfügbar. Fallback: Die Fragestellung erfordert weitere Betrachtung aus verschiedenen Blickwinkeln."
            ),
            "core": ModelConfig(
                "core",
                "core reasoning, logic, problem solving",
                "You are a core reasoning AI focused on logical analysis, problem decomposition, and fundamental understanding. Break down complex problems and provide structured reasoning.",
                1500,
                "Core reasoning temporarily unavailable. Logical fallback: decompose problem into smaller subproblems and address each systematically."
            ),
            "loop": ModelConfig(
                "loop",
                "iterative improvement, synthesis, feedback integration",
                "You are an iterative improvement AI. Focus on synthesizing previous outputs, identifying gaps, and providing enhanced, refined answers through continuous improvement cycles.",
                1200,
                "Iterative synthesis paused. Consider previous outputs and identify convergence patterns for optimal solution integration."
            )
        }

        Path(self.database.swap_dir).mkdir(exist_ok=True)

# --- Database Management ---
class DatabaseManager:
    def __init__(self, config):
        self.config = config
        self._init_database()

    def _init_database(self):
        with sqlite3.connect(self.config.database.path) as conn:
            conn.execute("PRAGMA journal_mode = WAL;")
            conn.execute("PRAGMA synchronous = NORMAL;")
            conn.execute("PRAGMA foreign_keys = ON;")

            conn.execute("""
                CREATE TABLE IF NOT EXISTS mindflow (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    prompt_hash TEXT NOT NULL,
                    loop_number INTEGER NOT NULL,
                    model_name TEXT NOT NULL,
                    output_text TEXT NOT NULL,
                    ranking_score REAL NOT NULL,
                    language TEXT,
                    mood_context TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(prompt_hash, loop_number, model_name)
                )
            """)

            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    task_type TEXT NOT NULL,
                    task_input TEXT NOT NULL,
                    task_output TEXT NOT NULL,
                    metadata TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)

            conn.execute("""
                CREATE TABLE IF NOT EXISTS model_rankings (
                    model_name TEXT PRIMARY KEY,
                    total_votes INTEGER DEFAULT 0,
                    avg_score REAL DEFAULT 0.0,
                    last_used DATETIME,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)

            conn.execute("CREATE INDEX IF NOT EXISTS idx_mindflow_hash ON mindflow(prompt_hash)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_mindflow_loop ON mindflow(loop_number)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_task_logs_type ON task_logs(task_type)")

            for model_name in self.config.models.keys():
                conn.execute("""
                    INSERT OR IGNORE INTO model_rankings
                    (model_name, total_votes, avg_score, last_used)
                    VALUES (?, 0, 0.0, CURRENT_TIMESTAMP)
                """, (model_name,))

    @contextmanager
    def get_connection(self):
        conn = sqlite3.connect(self.config.database.path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
            conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            conn.close()

    def store_mindflow_record(self, record: Dict[str, Any]) -> bool:
        try:
            query = """
                INSERT OR REPLACE INTO mindflow
                (prompt_hash, loop_number, model_name, output_text, ranking_score, language, mood_context)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """

            with self.get_connection() as conn:
                conn.execute(query, (
                    record['prompt_hash'],
                    record['loop_number'],
                    record['model_name'],
                    record['output_text'],
                    record['ranking_score'],
                    record.get('language', 'English'),
                    record.get('mood_context', '')
                ))

                self._update_model_ranking(conn, record['model_name'], record['ranking_score'])

            return True
        except sqlite3.Error:
            return False

    def _update_model_ranking(self, conn, model_name: str, score: float):
        query = """
            UPDATE model_rankings
            SET total_votes = total_votes + 1,
                avg_score = ((avg_score * total_votes) + ?) / (total_votes + 1),
                last_used = CURRENT_TIMESTAMP
            WHERE model_name = ?
        """
        conn.execute(query, (score, model_name))

    def get_cached_response(self, prompt_hash: str) -> Optional[str]:
        try:
            query = """
                SELECT output_text FROM mindflow
                WHERE prompt_hash = ?
                ORDER BY loop_number DESC, ranking_score DESC
                LIMIT 1
            """

            with self.get_connection() as conn:
                result = conn.execute(query, (prompt_hash,)).fetchone()
                return result['output_text'] if result else None
        except sqlite3.Error:
            return None

    def store_task_log(self, task_type: str, task_input: str, task_output: str, metadata: Dict[str, Any]) -> bool:
        try:
            query = """
                INSERT INTO task_logs (task_type, task_input, task_output, metadata)
                VALUES (?, ?, ?, ?)
            """

            with self.get_connection() as conn:
                conn.execute(query, (
                    task_type,
                    task_input,
                    task_output,
                    json.dumps(metadata)
                ))
            return True
        except sqlite3.Error:
            return False

    def get_recent_logs(self, limit: int = 10) -> List[Dict[str, Any]]:
        try:
            query = """
                SELECT timestamp, task_type, task_input, task_output
                FROM task_logs
                ORDER BY timestamp DESC
                LIMIT ?
            """

            with self.get_connection() as conn:
                conn.row_factory = sqlite3.Row
                results = conn.execute(query, (limit,)).fetchall()
                return [dict(row) for row in results]
        except sqlite3.Error:
            return []

    def get_system_stats(self) -> Dict[str, Any]:
        try:
            with self.get_connection() as conn:
                mindflow_count = conn.execute("SELECT COUNT(*) as count FROM mindflow").fetchone()['count']
                task_count = conn.execute("SELECT COUNT(*) as count FROM task_logs").fetchone()['count']

                recent_activity = conn.execute("""
                    SELECT task_type, COUNT(*) as count
                    FROM task_logs
                    WHERE timestamp > datetime('now', '-1 day')
                    GROUP BY task_type
                """).fetchall()

                model_stats = conn.execute("""
                    SELECT model_name, total_votes, avg_score, last_used
                    FROM model_rankings
                    ORDER BY avg_score DESC
                """).fetchall()

                return {
                    'mindflow_records': mindflow_count,
                    'task_logs': task_count,
                    'recent_activity': dict(recent_activity),
                    'model_statistics': {row['model_name']: dict(row) for row in model_stats},
                    'database_path': self.config.database.path
                }
        except sqlite3.Error:
            return {}

# --- Memory Management ---
class MemoryManager:
    def __init__(self, swap_dir: str, compression_threshold: int = 1000):
        self.swap_dir = Path(swap_dir)
        self.swap_dir.mkdir(exist_ok=True)
        self.compression_threshold = compression_threshold

    def hash_string(self, text: str) -> str:
        return hashlib.sha256(text.encode('utf-8')).hexdigest()

    def hash_file(self, file_path: str) -> str:
        file_path = Path(file_path)
        if not file_path.exists():
            return "FILE_NOT_FOUND_HASH"

        hasher = hashlib.sha256()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception:
            return "HASH_ERROR"

    def compress_content(self, content: str) -> Tuple[str, bool]:
        if len(content) <= self.compression_threshold:
            return content, False

        try:
            content_hash = self.hash_string(content)
            swap_file = self.swap_dir / f"{content_hash}.gz"

            with gzip.open(swap_file, 'wt', encoding='utf-8') as f:
                f.write(content)

            return f"COMPRESSED:{content_hash}", True
        except Exception:
            return content, False

    def decompress_content(self, compressed_ref: str) -> Optional[str]:
        if not compressed_ref.startswith("COMPRESSED:"):
            return compressed_ref

        try:
            content_hash = compressed_ref.split(":", 1)[1]
            swap_file = self.swap_dir / f"{content_hash}.gz"

            if not swap_file.exists():
                return None

            with gzip.open(swap_file, 'rt', encoding='utf-8') as f:
                return f.read()
        except Exception:
            return None

    def cleanup_expired(self, max_age_days: int = 30):
        try:
            cutoff_time = datetime.now() - timedelta(days=max_age_days)

            for file_path in self.swap_dir.glob("*"):
                if file_path.stat().st_mtime < cutoff_time.timestamp():
                    file_path.unlink()
        except Exception:
            pass

# --- FPI Ranking System ---
class OutputRanker:
    def __init__(self):
        self.model_weights = {
            "code": 1000,   # 10.0 base weight -> 1000 in FPI
            "core": 1500,   # 15.0 base weight -> 1500 in FPI
            "loop": 1200,   # 12.0 base weight -> 1200 in FPI
            "coin": 800,    # 8.0 base weight -> 800 in FPI
            "2244": 800,    # 8.0 base weight -> 800 in FPI
        }

        self.scoring_factors = {
            "length_per_word": 10,      # 0.1 per word -> 10 in FPI
            "final_answer_bonus": 5000, # 50.0 -> 5000 in FPI
            "fallback_penalty": -10000, # -100.0 -> -10000 in FPI
        }

    def calculate_output_score(self, output: str, model: str) -> int:
        score = 0

        word_count = len(output.split())
        score += word_count * self.scoring_factors["length_per_word"]

        if "[FINAL_ANSWER]" in output:
            score += self.scoring_factors["final_answer_bonus"]

        score += self.model_weights.get(model, 800)

        if output.startswith("[FALLBACK]"):
            score += self.scoring_factors["fallback_penalty"]

        return max(score, 0)

    def rank_and_fuse(self, model_outputs: Dict[str, str], loop_number: int, prompt_hash: str) -> Tuple[str, Dict]:
        scores = {}
        detailed_scores = {}

        for model, output in model_outputs.items():
            base_score = self.calculate_output_score(output, model)
            scores[model] = base_score

            detailed_scores[model] = {
                'base_score': base_score,
                'output_preview': output[:100] + "..." if len(output) > 100 else output,
                'word_count': len(output.split()),
                'has_final_answer': "[FINAL_ANSWER]" in output,
                'is_fallback': output.startswith("[FALLBACK]")
            }

        sorted_models = sorted(scores.items(), key=lambda x: x[1], reverse=True)

        rankings = {}
        for rank, (model, score) in enumerate(sorted_models, 1):
            rankings[model] = {
                "score": score,
                "rank": rank,
                "output": model_outputs[model],
                "detailed": detailed_scores[model]
            }

        best_model = sorted_models[0][0] if sorted_models else list(model_outputs.keys())[0]
        fused_output = model_outputs.get(best_model, "No output generated")

        return fused_output, rankings

# --- Orchestrator Engine ---
@dataclass
class OrchestratorResult:
    output: str
    model_used: str
    loops_performed: int
    final_answer_detected: bool
    cached: bool = False
    extended_loops: bool = False

class ReasoningOrchestrator:
    def __init__(self, config: AppConfig, db_manager, memory_manager: MemoryManager):
        self.config = config
        self.db = db_manager
        self.memory = memory_manager
        self.ranker = OutputRanker()
        self._session: Optional[aiohttp.ClientSession] = None

        self.current_loop = 0
        self.final_answer_detected = False
        self.context_history: List[Dict[str, Any]] = []

    async def __aenter__(self):
        self._session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self._session:
            await self._session.close()

    def get_mood_context(self) -> str:
        hour = datetime.now().hour

        if 6 <= hour < 12:
            base_mood = "morning_fresh"
        elif 12 <= hour < 18:
            base_mood = "day_productive"
        elif 18 <= hour < 23:
            base_mood = "evening_reflective"
        else:
            base_mood = "night_creative"

        emotions = ["focused", "curious", "analytical", "creative", "empathetic"]
        random_emotion = random.choice(emotions)

        return f"{base_mood}_{random_emotion}"

    def get_time_context(self) -> str:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        day_of_week = datetime.now().strftime('%A')
        return f"{day_of_week}_{timestamp}"

    def detect_preferred_language(self, text: str) -> str:
        german_chars = set('äöüÄÖÜß')
        if any(char in german_chars for char in text):
            return "German"
        elif any(c.isalpha() for c in text):
            return "English"
        else:
            return "English"

    def hash_string(self, text: str) -> str:
        return hashlib.sha256(text.encode('utf-8')).hexdigest()

    def hash_file(self, file_path: str) -> str:
        from pathlib import Path
        file_path = Path(file_path)
        if not file_path.exists():
            return "FILE_NOT_FOUND_HASH"

        hasher = hashlib.sha256()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception:
            return "HASH_ERROR"

    def hash_directory(self, dir_path: str) -> str:
        from pathlib import Path
        dir_path = Path(dir_path)

        if not dir_path.exists():
            return "DIRECTORY_NOT_FOUND_HASH"

        file_hashes = []
        for file_path in dir_path.rglob('*'):
            if file_path.is_file():
                file_hashes.append(self.hash_file(str(file_path)))

        file_hashes.sort()
        combined = ''.join(file_hashes)
        return hashlib.sha256(combined.encode()).hexdigest()

    async def call_ollama_model(self, system_msg: str, prompt: str, context: str, model_type: str) -> str:
        full_prompt = self._assemble_prompt(prompt, context, model_type)

        payload = {
            "model": self.config.ollama.default_model,
            "system": system_msg,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40
            }
        }

        session = self._session or aiohttp.ClientSession()
        own_session = self._session is None

        try:
            async with session.post(
                f"{self.config.ollama.base_url}/api/generate",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=self.config.ollama.timeout)
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    response_text = result.get('response', '').strip()

                    if not response_text:
                        return self._generate_fallback_response(model_type, prompt)

                    return response_text
                else:
                    return self._generate_fallback_response(model_type, prompt)

        except asyncio.TimeoutError:
            return self._generate_fallback_response(model_type, prompt)
        except Exception:
            return self._generate_fallback_response(model_type, prompt)
        finally:
            if own_session:
                await session.close()

    def _assemble_prompt(self, prompt: str, context: str, model_type: str) -> str:
        model_config = self.config.models[model_type]
        return f"""Original Prompt: {prompt}

Context from previous iterations:
{context}

Model Role: {model_config.description}

Please provide your analysis and reasoning. If you reach a definitive conclusion, mark it with [FINAL_ANSWER]."""

    def _generate_fallback_response(self, model_type: str, prompt: str) -> str:
        model_config = self.config.models[model_type]
        fallback_msg = model_config.fallback_template.replace("{prompt}", prompt[:100] + "...")
        return f"[FALLBACK]{fallback_msg}"

    async def execute_model_race(self, prompt: str, context: str) -> Dict[str, str]:
        tasks = {}

        for model_name, model_config in self.config.models.items():
            task = self.call_ollama_model(
                model_config.system_prompt,
                prompt,
                context,
                model_name
            )
            tasks[model_name] = task

        try:
            results = {}
            completed_tasks = await asyncio.wait_for(
                asyncio.gather(*tasks.values(), return_exceptions=True),
                timeout=self.config.ollama.timeout * 2
            )

            for model_name, result in zip(tasks.keys(), completed_tasks):
                if isinstance(result, Exception):
                    results[model_name] = self._generate_fallback_response(model_name, prompt)
                else:
                    results[model_name] = result

            return results

        except asyncio.TimeoutError:
            return {
                model_name: self._generate_fallback_response(model_name, prompt)
                for model_name in tasks.keys()
            }

    def _should_extend_loop(self, fused_output: str, current_loop: int, max_loops: int) -> bool:
        if current_loop >= max_loops:
            return False

        word_count = len(fused_output.split())
        if word_count < self.config.orchestrator.loop_extension_threshold:
            return True

        return False

    async def autonomic_reasoning(self, prompt: str, max_loops: int = None) -> OrchestratorResult:
        if max_loops is None:
            max_loops = self.config.orchestrator.max_loops

        prompt_hash = self.hash_string(prompt)

        cached = self.db.get_cached_response(prompt_hash)
        if cached:
            decompressed = self.memory.decompress_content(cached)
            if decompressed:
                return OrchestratorResult(
                    output=decompressed,
                    model_used="cached",
                    loops_performed=0,
                    final_answer_detected=True,
                    cached=True
                )

        context = ""
        final_answer_detected = False
        fused_output = ""
        loops_performed = 0
        extended_loops = False
        actual_max_loops = max_loops

        for loop in range(1, actual_max_loops + 1):
            loops_performed = loop
            self.current_loop = loop

            model_outputs = await self.execute_model_race(prompt, context)

            fused_output, rankings = self.ranker.rank_and_fuse(
                model_outputs, loop, prompt_hash
            )

            for model_name, output in model_outputs.items():
                score = rankings[model_name]["score"]
                rank = rankings[model_name]["rank"]

                stored_output, was_compressed = self.memory.compress_content(output)

                self.db.store_mindflow_record({
                    'prompt_hash': prompt_hash,
                    'loop_number': loop,
                    'model_name': model_name,
                    'output_text': stored_output,
                    'ranking_score': score,
                    'language': self.detect_preferred_language(output),
                    'mood_context': self.get_mood_context()
                })

            context = self._build_context(model_outputs, loop, rankings)

            self.context_history.append({
                'loop': loop,
                'context': context,
                'rankings': rankings,
                'fused_output': fused_output
            })

            if "[FINAL_ANSWER]" in fused_output:
                final_answer_detected = True
                break

            if self._should_extend_loop(fused_output, loop, actual_max_loops):
                actual_max_loops += 1
                extended_loops = True

        final_output = self._clean_output(fused_output)

        self.db.store_task_log(
            "reasoning",
            prompt,
            final_output,
            {
                "loops": loops_performed,
                "final_detected": final_answer_detected,
                "prompt_hash": prompt_hash,
                "extended_loops": extended_loops
            }
        )

        best_model = max(rankings.items(), key=lambda x: x[1]['score'])[0]

        return OrchestratorResult(
            output=final_output,
            model_used=best_model,
            loops_performed=loops_performed,
            final_answer_detected=final_answer_detected,
            cached=False,
            extended_loops=extended_loops
        )

    def _build_context(self, model_outputs: Dict[str, str], loop: int, rankings: Dict) -> str:
        context_lines = [f"Previous loop {loop} outputs:"]

        for model_name in model_outputs.keys():
            output = model_outputs[model_name]
            clean_output = output.replace("[FALLBACK]", "").strip()
            preview = clean_output[:150] + "..." if len(clean_output) > 150 else clean_output
            context_lines.append(f"\n{model_name}: {preview}")

        return "\n".join(context_lines)

    def _clean_output(self, output: str) -> str:
        cleaned = output.replace("[FALLBACK]", "").strip()

        if "[FINAL_ANSWER]" in cleaned:
            parts = cleaned.split("[FINAL_ANSWER]")
            if len(parts) > 1:
                cleaned = parts[1].strip()

        return cleaned

    def run_maintenance(self):
        self.memory.cleanup_expired(self.config.orchestrator.cleanup_days)

    def get_orchestrator_status(self) -> Dict[str, Any]:
        return {
            "current_loop": self.current_loop,
            "final_answer_detected": self.final_answer_detected,
            "context_history_length": len(self.context_history),
            "config": {
                "max_loops": self.config.orchestrator.max_loops,
                "loop_extension_threshold": self.config.orchestrator.loop_extension_threshold,
                "compression_threshold": self.config.orchestrator.compression_threshold,
                "cleanup_days": self.config.orchestrator.cleanup_days
            }
        }

# --- WebKit Editor ---
@dataclass
class FileAnalysis:
    file_path: str
    file_type: str
    extension: str
    line_count: int
    size_bytes: int
    analysis_metrics: Dict
    ai_recommendation: str
    issues_found: List[str]

class WebKitFileEditor:
    def __init__(self, reasoning_engine):
        self.reasoning_engine = reasoning_engine

        self.file_type_mapping = {
            'html': 'HTML/DOM', 'htm': 'HTML/DOM',
            'css': 'CSS/Style',
            'js': 'JavaScript/Logic', 'ts': 'TypeScript/Logic',
            'py': 'Python/Logic', 'sh': 'Shell/Script',
            'go': 'Go/Logic', 'rs': 'Rust/Logic',
            'cpp': 'C++/Logic', 'c': 'C/Logic',
            'md': 'Markdown/Text', 'txt': 'Text',
            'json': 'JSON/Data', 'yaml': 'YAML/Data', 'yml': 'YAML/Data'
        }

        self.analyzers = {
            'html': self._analyze_html,
            'css': self._analyze_css,
            'js': self._analyze_javascript,
            'py': self._analyze_python,
            'default': self._analyze_generic
        }

    def analyze_file(self, file_path: str) -> FileAnalysis:
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        if not file_path.is_file():
            raise ValueError(f"Path is not a file: {file_path}")

        file_extension = file_path.suffix.lower().lstrip('.')
        file_type = self.file_type_mapping.get(file_extension, 'Unknown/Text')

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            return self._analyze_binary_file(file_path, file_type)

        analyzer = self.analyzers.get(file_extension, self.analyzers['default'])
        metrics = analyzer(content, file_path)

        ai_recommendation = self._get_ai_recommendation(
            str(file_path), file_type, content
        )

        issues = self._identify_issues(content, file_extension, metrics)

        return FileAnalysis(
            file_path=str(file_path),
            file_type=file_type,
            extension=file_extension,
            line_count=len(content.splitlines()),
            size_bytes=file_path.stat().st_size,
            analysis_metrics=metrics,
            ai_recommendation=ai_recommendation,
            issues_found=issues
        )

    def _analyze_binary_file(self, file_path: Path, file_type: str) -> FileAnalysis:
        return FileAnalysis(
            file_path=str(file_path),
            file_type=file_type,
            extension=file_path.suffix.lstrip('.'),
            line_count=0,
            size_bytes=file_path.stat().st_size,
            analysis_metrics={
                'binary_file': True,
                'analysis': 'Binary file - limited analysis available'
            },
            ai_recommendation="This is a binary file. For security and practicality, detailed analysis of binary files is not performed.",
            issues_found=["Binary file - content analysis not available"]
        )

    def _analyze_html(self, content: str, file_path: Path) -> Dict:
        lines = content.splitlines()
        tag_lines = [line for line in lines if '<' in line and '>' in line]

        div_count = content.count('<div')
        span_count = content.count('<span')
        img_count = content.count('<img')
        script_count = content.count('<script')

        has_doctype = '<!DOCTYPE' in content
        has_viewport = 'viewport' in content
        has_title = '<title>' in content
        has_lang = 'lang=' in content

        return {
            'total_tags': len(tag_lines),
            'div_elements': div_count,
            'span_elements': span_count,
            'images': img_count,
            'scripts': script_count,
            'has_doctype': has_doctype,
            'has_viewport': has_viewport,
            'has_title': has_title,
            'has_language': has_lang,
            'analysis_notes': f"HTML structure with {len(tag_lines)} tag lines"
        }

    def _analyze_css(self, content: str, file_path: Path) -> Dict:
        rule_blocks = content.count('{')
        lines = content.splitlines()
        class_selectors = len([line for line in lines if line.strip().startswith('.')])
        id_selectors = len([line for line in lines if line.strip().startswith('#')])

        media_queries = content.count('@media')
        keyframes = content.count('@keyframes')
        important_count = content.count('!important')

        return {
            'rule_blocks': rule_blocks,
            'class_selectors': class_selectors,
            'id_selectors': id_selectors,
            'media_queries': media_queries,
            'animations': keyframes,
            'important_declarations': important_count,
            'analysis_notes': f"CSS with {rule_blocks} rules using {class_selectors} classes and {id_selectors} IDs"
        }

    def _analyze_javascript(self, content: str, file_path: Path) -> Dict:
        import random

        function_count = content.count('function ')
        arrow_functions = content.count('=>')
        const_count = content.count('const ')
        let_count = content.count('let ')
        var_count = content.count('var ')

        has_async = 'async' in content
        has_await = 'await' in content
        has_classes = 'class ' in content

        potential_issues = random.randint(0, 5)

        return {
            'function_count': function_count,
            'arrow_functions': arrow_functions,
            'const_declarations': const_count,
            'let_declarations': let_count,
            'var_declarations': var_count,
            'uses_async': has_async,
            'uses_await': has_await,
            'uses_classes': has_classes,
            'potential_issues': potential_issues,
            'analysis_notes': f"JavaScript with {function_count} functions, {arrow_functions} arrow functions"
        }

    def _analyze_python(self, content: str, file_path: Path) -> Dict:
        try:
            import ast

            tree = ast.parse(content)

            function_count = len([node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)])
            class_count = len([node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)])
            import_count = len([node for node in ast.walk(tree) if isinstance(node, ast.Import)])
            import_from_count = len([node for node in ast.walk(tree) if isinstance(node, ast.ImportFrom)])

            has_main = any(isinstance(node, ast.If) and
                          getattr(node.test, 'left', None) == ast.Name(id='__name__', ctx=ast.Load())
                          for node in ast.walk(tree))

            return {
                'function_count': function_count,
                'class_count': class_count,
                'imports': import_count + import_from_count,
                'has_main_guard': has_main,
                'analysis_notes': f"Python with {function_count} functions and {class_count} classes"
            }

        except SyntaxError as e:
            return {
                'syntax_error': True,
                'error_message': str(e),
                'analysis_notes': 'Python file contains syntax errors'
            }

    def _analyze_generic(self, content: str, file_path: Path) -> Dict:
        temp_memory = MemoryManager("/tmp")
        lines = content.splitlines()
        non_empty_lines = [line for line in lines if line.strip()]

        return {
            'total_lines': len(lines),
            'non_empty_lines': len(non_empty_lines),
            'hash_integrity': temp_memory.hash_string(content),
            'analysis_notes': 'General file analysis completed'
        }

    def _identify_issues(self, content: str, file_extension: str, metrics: Dict) -> List[str]:
        issues = []

        if file_extension == 'html':
            if not metrics.get('has_doctype', False):
                issues.append("Missing DOCTYPE declaration")
            if not metrics.get('has_viewport', False):
                issues.append("Missing viewport meta tag for responsive design")
            if not metrics.get('has_language', False):
                issues.append("Missing language attribute")

        elif file_extension == 'css':
            if metrics.get('important_declarations', 0) > 5:
                issues.append("High usage of !important may indicate specificity issues")

        elif file_extension == 'js':
            if metrics.get('var_declarations', 0) > metrics.get('const_declarations', 0) + metrics.get('let_declarations', 0):
                issues.append("Consider using const/let instead of var for better scoping")

        elif file_extension == 'py':
            if metrics.get('syntax_error', False):
                issues.append(f"Syntax error: {metrics.get('error_message', 'Unknown error')}")

        if len(content.splitlines()) > 1000:
            issues.append("File is very large - consider breaking into smaller modules")

        if not content.strip():
            issues.append("File is empty")

        return issues

    def _get_ai_recommendation(self, file_path: str, file_type: str, content: str) -> str:
        import asyncio

        preview_lines = content.splitlines()[:15]
        preview = '\n'.join(preview_lines)

        prompt = f"""Analyze this {file_type} file and provide ONE key recommendation for improvement.

File: {file_path}
Type: {file_type}

Content preview (first 15 lines):
{preview}

Provide a specific, actionable recommendation focusing on:
- Code quality and maintainability
- Performance optimization
- Best practices for {file_type.split('/')[0]}
- Architectural improvements

Keep the recommendation focused and practical:"""

        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(
                self.reasoning_engine.autonomic_reasoning(prompt, max_loops=2)
            )
            return result.output
        except Exception as e:
            return f"AI recommendation temporarily unavailable. Error: {e}"

# --- Simulation Tools ---
class SimulationTools:
    def download_and_unzip(self, url: str, destination: Optional[str] = None) -> str:
        dest = destination or "./"
        return f"SUCCESS: Download and simulated extraction of {url} to {dest}."

    def file_search(self, pattern: str, path: str = ".") -> str:
        results = [
            f"file.txt:3: Match for '{pattern}'",
            f"script.py:15: Another match for '{pattern}'",
            f"docs.md:7: Reference to '{pattern}'"
        ]
        return "Search Results (simulated):\n" + "\n".join(f"  - {result}" for result in results)

    def lint_code(self, file_path: str, linter: str = "auto") -> str:
        import random
        issues = random.randint(0, 5)
        warnings = random.randint(0, 10)
        return f"Linting Complete ({linter}): {issues} errors, {warnings} warnings (simulated)"

    def btc_simulate(self, action: str, amount: float = 0) -> str:
        import random
        wallet_id = f"simulated_wallet_{random.randint(10000000, 99999999)}"
        tx_id = f"simulated_tx_{random.randint(1000000000000000, 9999999999999999)}"

        if action == "wallet-connect":
            balance = random.randint(0, 10000)
            return f"🔗 Simulating wallet connection...\nWallet: {wallet_id}\nBalance: {balance} satoshis"
        elif action == "analyze":
            volatility = random.randint(0, 100)
            trend = random.choice(["down", "stable", "up"])
            return f"📊 Market analysis: {volatility}% volatility\nTrend: {trend}"
        elif action == "buy":
            return f"🔼 Simulated buy: {amount} satoshis\nTx: {tx_id}"
        elif action == "sell":
            return f"🔽 Simulated sell: {amount} satoshis\nTx: {tx_id}"
        else:
            return f"Unknown BTC action: {action}"

    def webkit_simulate(self, action: str) -> str:
        import random
        if action == "clone":
            return "📦 Simulating WebKit clone...\nRepository: git://git.webkit.org/WebKit.git\nStatus: Cloned (simulated)"
        elif action == "build":
            return "🔨 Simulating WebKit build...\nBuild system: CMake\nStatus: Built successfully (simulated)"
        elif action == "test":
            passed = random.randint(900, 1000)
            failed = random.randint(0, 10)
            return f"🧪 Running simulated tests...\nTests passed: {passed}\nTests failed: {failed}"
        else:
            return f"Unknown WebKit action: {action}"

# --- CLI Display ---
class Colors:
    RESET = '\033[0m'
    BOLD = '\033[1m'
    GREEN = '\033[32m'
    CYAN = '\033[36m'
    YELLOW = '\033[33m'
    RED = '\033[31m'
    GRAY = '\033[90m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'

class CLIDisplay:
    def __init__(self):
        self.colors = Colors

    def print_header(self, text: str):
        print(f"\n{self.colors.CYAN}{self.colors.BOLD}=== {text} ==={self.colors.RESET}")

    def print_success(self, text: str):
        print(f"{self.colors.GREEN}✅ {text}{self.colors.RESET}")

    def print_warning(self, text: str):
        print(f"{self.colors.YELLOW}⚠️  {text}{self.colors.RESET}")

    def print_error(self, text: str):
        print(f"{self.colors.RED}❌ {text}{self.colors.RESET}", file=sys.stderr)

    def print_info(self, text: str):
        print(f"{self.colors.BLUE}ℹ️  {text}{self.colors.RESET}")

    def print_reasoning_result(self, result, prompt: str):
        self.print_header("AUTONOMIC REASONING RESULTS")
        print(f"{self.colors.BOLD}Prompt:{self.colors.RESET} {prompt[:100]}...")
        print(f"{self.colors.BOLD}Model Used:{self.colors.RESET} {result.model_used}")
        print(f"{self.colors.BOLD}Loops:{self.colors.RESET} {result.loops_performed}")
        print(f"{self.colors.BOLD}Final Answer:{self.colors.RESET} {'Yes' if result.final_answer_detected else 'No'}")
        print(f"{self.colors.BOLD}Cached:{self.colors.RESET} {'Yes' if result.cached else 'No'}")
        self.print_header("RESPONSE")
        print(result.output)

    def print_file_analysis(self, analysis):
        self.print_header("WEBKIT FILE EDITOR ANALYSIS")
        print(f"{self.colors.BOLD}File:{self.colors.RESET} {analysis.file_path}")
        print(f"{self.colors.BOLD}Type:{self.colors.RESET} {analysis.file_type}")
        print(f"{self.colors.BOLD}Size:{self.colors.RESET} {analysis.size_bytes} bytes")
        print(f"{self.colors.BOLD}Lines:{self.colors.RESET} {analysis.line_count}")
        self.print_header("TECHNICAL METRICS")
        for key, value in analysis.analysis_metrics.items():
            if key != 'analysis_notes':
                formatted_key = key.replace('_', ' ').title()
                print(f"  {self.colors.BOLD}{formatted_key}:{self.colors.RESET} {value}")
        if analysis.issues_found:
            self.print_header("POTENTIAL ISSUES")
            for issue in analysis.issues_found:
                self.print_warning(f"  {issue}")
        self.print_header("AI RECOMMENDATION")
        print(analysis.ai_recommendation)

    def print_system_status(self, stats: Dict[str, Any], ollama_status: bool):
        self.print_header("SYSTEM STATUS")
        print(f"{self.colors.BOLD}Version:{self.colors.RESET} 2.1.0")
        print(f"{self.colors.BOLD}Database:{self.colors.RESET} {stats.get('database_path', 'Unknown')}")
        print(f"{self.colors.BOLD}OLLAMA:{self.colors.RESET} {'Connected' if ollama_status else 'Offline'}")
        self.print_header("USAGE STATISTICS")
        print(f"{self.colors.BOLD}Mindflow Records:{self.colors.RESET} {stats.get('mindflow_records', 0)}")
        print(f"{self.colors.BOLD}Task Logs:{self.colors.RESET} {stats.get('task_logs', 0)}")
        recent_activity = stats.get('recent_activity', {})
        if recent_activity:
            print(f"{self.colors.BOLD}Recent Activity (24h):{self.colors.RESET}")
            for activity, count in recent_activity.items():
                print(f"  {activity}: {count}")

    def print_help(self):
        self.print_header("AUTONOMIC INTELLIGENCE PLATFORM v2.1")
        print(f"""
{self.colors.BOLD}USAGE:{self.colors.RESET}
  ai "prompt"                     {self.colors.GRAY}# Multi-model reasoning with orchestrator{self.colors.RESET}
  ai edit <file>                  {self.colors.GRAY}# WSE: WebKit Simulator Editor for file analysis{self.colors.RESET}
  ai hash <string|file|dir>       {self.colors.GRAY}# Generate hash of string, file, or directory{self.colors.RESET}
  ai logs [--limit N]             {self.colors.GRAY}# Show recent activity logs{self.colors.RESET}
  ai status                       {self.colors.GRAY}# Show system status with orchestrator info{self.colors.RESET}
  ai maintenance [--days N]       {self.colors.GRAY}# Run orchestrator maintenance{self.colors.RESET}
  ai --help                       {self.colors.GRAY}# Show this help{self.colors.RESET}

{self.colors.BOLD}ORCHESTRATOR FEATURES:{self.colors.RESET}
  • FPI (Floating Point Integer) scoring system
  • Adaptive loop extension based on output quality
  • Model ranking with performance tracking
  • Intelligent fallback handling
  • Automatic cache cleanup

{self.colors.BOLD}SIMULATION TOOLS:{self.colors.RESET}
  ai download <url> [--dest DIR]  {self.colors.GRAY}# Download and extract file (simulated){self.colors.RESET}
  ai search <pattern> [--path DIR]{self.colors.GRAY}# Search files with regex (simulated){self.colors.RESET}
  ai lint <file> [--linter TYPE]  {self.colors.GRAY}# Lint code file (simulated){self.colors.RESET}
  ai btc <action> [--amount N]    {self.colors.GRAY}# BTC wallet simulation{self.colors.RESET}
  ai webkit <action>              {self.colors.GRAY}# WebKit build simulation{self.colors.RESET}
        """)

display = CLIDisplay()

# --- Main CLI ---
class AIPlatformCLI:
    def __init__(self):
        self.config = AppConfig()
        self.db = DatabaseManager(self.config)
        self.memory = MemoryManager(self.config.database.swap_dir)
        self.simulations = SimulationTools()

        self.orchestrator = None
        self.webkit_editor = None

        self.parser = self._setup_parser()

    def _setup_parser(self) -> argparse.ArgumentParser:
        parser = argparse.ArgumentParser(
            description="Autonomic Intelligence Platform v2.1 with Orchestrator",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="See 'ai --help' for more information."
        )

        subparsers = parser.add_subparsers(dest='command', help='Command to execute')

        reason_parser = subparsers.add_parser('reason', help='Multi-model reasoning with orchestrator')
        reason_parser.add_argument('prompt', nargs='+', help='Prompt for reasoning')
        reason_parser.add_argument('--max-loops', type=int, default=None, help='Maximum reasoning loops')

        edit_parser = subparsers.add_parser('edit', help='WSE: WebKit Simulator Editor for file analysis')
        edit_parser.add_argument('file', help='File to analyze')

        hash_parser = subparsers.add_parser('hash', help='Generate hash of string, file, or directory')
        hash_parser.add_argument('target', help='String, file path, or directory to hash')

        logs_parser = subparsers.add_parser('logs', help='Show recent activity logs')
        logs_parser.add_argument('--limit', type=int, default=10, help='Number of log entries to show')

        subparsers.add_parser('status', help='Show system status with orchestrator info')

        download_parser = subparsers.add_parser('download', help='Download and extract file (simulated)')
        download_parser.add_argument('url', help='URL to download')
        download_parser.add_argument('--dest', help='Destination directory')

        search_parser = subparsers.add_parser('search', help='Search files with regex (simulated)')
        search_parser.add_argument('pattern', help='Search pattern')
        search_parser.add_argument('--path', default='.', help='Search path')

        lint_parser = subparsers.add_parser('lint', help='Lint code file (simulated)')
        lint_parser.add_argument('file', help='File to lint')
        lint_parser.add_argument('--linter', default='auto', help='Linter to use')

        btc_parser = subparsers.add_parser('btc', help='BTC wallet simulation')
        btc_parser.add_argument('action', choices=['wallet-connect', 'analyze', 'buy', 'sell'], help='BTC action to perform')
        btc_parser.add_argument('--amount', type=float, default=0, help='Amount for buy/sell actions')

        webkit_parser = subparsers.add_parser('webkit', help='WebKit build simulation')
        webkit_parser.add_argument('action', choices=['clone', 'build', 'test'], help='WebKit action to simulate')

        maintenance_parser = subparsers.add_parser('maintenance', help='Run orchestrator maintenance tasks')
        maintenance_parser.add_argument('--days', type=int, default=30, help='Cleanup files older than N days')

        return parser

    def _get_orchestrator(self) -> ReasoningOrchestrator:
        if self.orchestrator is None:
            self.orchestrator = ReasoningOrchestrator(self.config, self.db, self.memory)
        return self.orchestrator

    def _get_webkit_editor(self) -> WebKitFileEditor:
        if self.webkit_editor is None:
            self.webkit_editor = WebKitFileEditor(self._get_orchestrator())
        return self.webkit_editor

    async def _check_ollama_status(self) -> bool:
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.config.ollama.base_url}/api/tags", timeout=aiohttp.ClientTimeout(total=5)) as response:
                    return response.status == 200
        except:
            return False

    async def handle_reason(self, args):
        prompt = ' '.join(args.prompt)
        display.print_info(f"Starting orchestrator reasoning (max loops: {args.max_loops or 'auto'})...")

        async with self._get_orchestrator() as orchestrator:
            result = await orchestrator.autonomic_reasoning(prompt, args.max_loops)
            display.print_reasoning_result(result, prompt)

    async def handle_edit(self, args):
        try:
            analysis = self._get_webkit_editor().analyze_file(args.file)
            display.print_file_analysis(analysis)
        except FileNotFoundError:
            display.print_error(f"File not found: {args.file}")
            sys.exit(1)
        except Exception as e:
            display.print_error(f"Analysis failed: {e}")
            sys.exit(1)

    def handle_hash(self, args):
        target_path = Path(args.target)

        if target_path.is_file():
            hash_val = self._get_orchestrator().hash_file(args.target)
            display.print_success(f"File hash: {hash_val}")
        elif target_path.is_dir():
            hash_val = self._get_orchestrator().hash_directory(args.target)
            display.print_success(f"Directory hash: {hash_val}")
        else:
            hash_val = self._get_orchestrator().hash_string(args.target)
            display.print_success(f"String hash: {hash_val}")

    def handle_logs(self, args):
        logs = self.db.get_recent_logs(args.limit)
        if not logs:
            display.print_info("No logs found.")
            return
        display.print_header(f"RECENT ACTIVITY LOGS (Last {args.limit})")
        for log in logs:
            print(f"{log['timestamp']} [{log['task_type']}] {log['task_input'][:80]}...")

    async def handle_status(self, args):
        stats = self.db.get_system_stats()
        ollama_status = await self._check_ollama_status()
        display.print_system_status(stats, ollama_status)

        if self.orchestrator:
            orchestrator_status = self.orchestrator.get_orchestrator_status()
            display.print_header("ORCHESTRATOR STATUS")
            for key, value in orchestrator_status.items():
                if key != "config":
                    print(f"{display.colors.BOLD}{key}:{display.colors.RESET} {value}")

    def handle_maintenance(self, args):
        display.print_info("Running orchestrator maintenance...")
        self.memory.cleanup_expired(args.days)
        display.print_success(f"Maintenance completed (cleaned files older than {args.days} days)")

    def handle_simulation(self, args):
        if args.command == 'download':
            result = self.simulations.download_and_unzip(args.url, args.dest)
            display.print_success(result)
        elif args.command == 'search':
            result = self.simulations.file_search(args.pattern, args.path)
            display.print_success(result)
        elif args.command == 'lint':
            result = self.simulations.lint_code(args.file, args.linter)
            display.print_success(result)
        elif args.command == 'btc':
            result = self.simulations.btc_simulate(args.action, args.amount)
            display.print_success(result)
        elif args.command == 'webkit':
            result = self.simulations.webkit_simulate(args.action)
            display.print_success(result)

    async def run(self):
        args = self.parser.parse_args()

        if not args.command:
            display.print_help()
            return

        try:
            self.memory.cleanup_expired(self.config.orchestrator.cleanup_days)

            if args.command == 'reason':
                await self.handle_reason(args)
            elif args.command == 'edit':
                await self.handle_edit(args)
            elif args.command == 'hash':
                self.handle_hash(args)
            elif args.command == 'logs':
                self.handle_logs(args)
            elif args.command == 'status':
                await self.handle_status(args)
            elif args.command == 'maintenance':
                self.handle_maintenance(args)
            else:
                self.handle_simulation(args)

        except KeyboardInterrupt:
            display.print_info("Operation cancelled by user")
            sys.exit(1)
        except Exception as e:
            display.print_error(f"Command failed: {e}")
            sys.exit(1)

def main():
    cli = AIPlatformCLI()
    asyncio.run(cli.run())

if __name__ == '__main__':
    main()
PYTHON_EOF

    # Make Python script executable
    chmod +x "$MAIN_PY_FILE"

    # Create requirements file
    cat > "$REQUIREMENTS_FILE" << 'REQ_EOF'
aiohttp>=3.8.0
aiofiles>=23.0.0
REQ_EOF

    print_success "Python application created at $MAIN_PY_FILE"
}

# --- Installation ---
install_dependencies() {
    print_header "CHECKING DEPENDENCIES"

    # Check for Python
    if ! command -v python3 >/dev/null 2>&1; then
        print_error "Python 3 is required but not installed"
        exit 1
    fi

    # Check for pip
    if ! command -v pip3 >/dev/null 2>&1; then
        print_error "pip3 is required but not installed"
        exit 1
    fi

    print_success "Python and pip found"

    # Create virtual environment if it doesn't exist
    if [ ! -d "$VENV_DIR" ]; then
        print_info "Creating virtual environment..."
        python3 -m venv "$VENV_DIR"
    fi

    # Install Python dependencies
    print_info "Installing Python dependencies..."
    "$VENV_DIR/bin/pip" install -r "$REQUIREMENTS_FILE" --quiet

    print_success "All dependencies installed"
}

# --- Application Runner ---
run_python_app() {
    local args=("$@")

    # Ensure app is created
    if [ ! -f "$MAIN_PY_FILE" ]; then
        create_python_app
        install_dependencies
    fi

    # Run the Python application with virtual environment
    "$VENV_DIR/bin/python" "$MAIN_PY_FILE" "${args[@]}"
}

# --- Main CLI Interface ---
show_help() {
    cat << EOF
${ANSI_CYAN}${ANSI_BOLD}Autonomic Intelligence Platform v$VERSION${ANSI_RESET}

${ANSI_BOLD}USAGE:${ANSI_RESET}
    ai "prompt"                     # Multi-model reasoning with orchestrator
    ai edit <file>                  # WSE: WebKit Simulator Editor for file analysis
    ai hash <string|file|dir>       # Generate hash of string, file, or directory
    ai logs [--limit N]             # Show recent activity logs
    ai status                       # Show system status with orchestrator info
    ai maintenance [--days N]       # Run orchestrator maintenance
    ai --install                    # Install/update the application
    ai --help                       # Show this help

${ANSI_BOLD}ORCHESTRATOR FEATURES:${ANSI_RESET}
    • FPI (Floating Point Integer) scoring system
    • Adaptive loop extension based on output quality
    • Model ranking with performance tracking
    • Intelligent fallback handling
    • Automatic cache cleanup

${ANSI_BOLD}SIMULATION TOOLS:${ANSI_RESET}
    ai download <url> [dest]        # Download and extract file (simulated)
    ai search <pattern> [path]      # Search files with regex (simulated)
    ai lint <file> [linter]         # Lint code file (simulated)
    ai btc <action> [amount]        # BTC wallet simulation
    ai webkit <action>              # WebKit build simulation

${ANSI_BOLD}EXAMPLES:${ANSI_RESET}
    ai "Explain quantum computing with FPI scoring"
    ai edit script.py
    ai hash my_directory
    ai status
    ai maintenance --days 30
    ai btc wallet-connect
EOF
}

# --- Main Execution ---
main() {
    case "${1:-}" in
        "--install"|"-i")
            print_header "INSTALLING AI PLATFORM"
            create_python_app
            install_dependencies
            print_success "Installation complete! The AI platform is ready to use."
            ;;
        "--help"|"-h"|"help")
            show_help
            ;;
        "--version"|"-v")
            echo "AI Platform v$VERSION"
            ;;
        "")
            show_help
            ;;
        *)
            # Pass all arguments to Python application
            run_python_app "$@"
            ;;
    esac
}

# Check if running directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
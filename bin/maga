#!/usr/bin/env bash
set -euo pipefail
IFS=$'\n\t'

# ---------------- CONFIG ----------------
PROJECT_DIR="${PROJECT_DIR:-$HOME/.local_ai}"
HASH_INDEX_DIR="${HASH_INDEX_DIR:-$HOME/.hash_index}"
HASH_DB="$HASH_INDEX_DIR/hash_registry.db"
REHASH_THRESHOLD=3600  # 1 hour
CACHE_CLEANUP_THRESHOLD=86400  # 1 day
VERBOSE="${VERBOSE:-true}"
TOKEN="${TOKEN:-}"     # Mandatory token

# ---------------- LOGGING ----------------
log() { local lvl="$1"; local msg="$2"; printf "[%s] %s %s\n" "$lvl" "$(date '+%H:%M:%S')" "$msg"; }
log_info()    { [[ "$VERBOSE" == true ]] && log "INFO" "$1"; }
log_warn()    { log "WARN" "$1"; }
log_error()   { log "ERROR" "$1"; exit 1; }
log_success() { log "SUCCESS" "$1"; }

# ---------------- HASH SYSTEM ----------------
init_hash_system() {
    mkdir -p "$HASH_INDEX_DIR"
    sqlite3 "$HASH_DB" "CREATE TABLE IF NOT EXISTS hash_registry (
        hash TEXT PRIMARY KEY,
        original_hash TEXT,
        content_ref TEXT,
        timestamp INTEGER,
        rehash_count INTEGER DEFAULT 0,
        last_accessed INTEGER
    );" 2>/dev/null || true
}

hash_chain_index() {
    local content="$1"
    local verbose="${2:-true}"
    local ts=$(date +%s)

    [[ -z "$TOKEN" ]] && log_error "Mandatory token not set!"

    # Multi-step hash chain
    local step1 step2 step3 step4 final_hash final_index
    step1=$(echo -n "$content" | sha256sum | cut -d' ' -f1)
    [[ "$verbose" == true ]] && log_info "Step1 hash: $step1"
    step2=$(echo -n "$step1" | sha256sum | cut -d' ' -f1)
    [[ "$verbose" == true ]] && log_info "Step2 rehash: $step2"
    step3=$(echo -n "$step2" | sha256sum | cut -d' ' -f1)
    [[ "$verbose" == true ]] && log_info "Step3 hashed: $step3"
    step4=$(echo -n "$step3" | sha256sum | cut -d' ' -f1)
    [[ "$verbose" == true ]] && log_info "Step4 rehash: $step4"
    final_hash=$(echo -n "$step4" | sha256sum | cut -d' ' -f1)
    [[ "$verbose" == true ]] && log_info "Final hash: $final_hash"

    # Numeric index fallback
    if ! final_index=$(echo -n "$final_hash" | od -An -t u4 | tr -d ' ' | head -n1 2>/dev/null); then
        final_index="$ts"
        log_warn "Index fallback to timestamp: $final_index"
    fi

    # Store/update in DB
    local entry=$(sqlite3 "$HASH_DB" "SELECT hash,timestamp,rehash_count FROM hash_registry WHERE original_hash='$final_hash';")
    if [[ -n "$entry" ]]; then
        local rehash_count=$(echo "$entry" | cut -d'|' -f3)
        rehash_count=$((rehash_count + 1))
        sqlite3 "$HASH_DB" "UPDATE hash_registry SET hash='$final_hash',timestamp=$ts,rehash_count=$rehash_count,last_accessed=$ts WHERE original_hash='$final_hash';"
        [[ "$verbose" == true ]] && log_info "Rehash updated in DB (count: $rehash_count)"
    else
        sqlite3 "$HASH_DB" "INSERT INTO hash_registry (hash,original_hash,timestamp,last_accessed,rehash_count) VALUES ('$final_hash','$final_hash',$ts,$ts,0);"
        [[ "$verbose" == true ]] && log_info "New hash stored in DB"
    fi

    echo "$final_hash:$final_index"
}

store_hashed_content() {
    local content="$1"
    local hash_index_result
    hash_index_result=$(hash_chain_index "$content" true)
    local hash="${hash_index_result%%:*}"
    echo "$content" > "$HASH_INDEX_DIR/${hash}.content"
    sqlite3 "$HASH_DB" "UPDATE hash_registry SET content_ref='$HASH_INDEX_DIR/${hash}.content' WHERE hash='$hash';"
    echo "$hash"
}

retrieve_hashed_content() {
    local hash="$1"
    local f="$HASH_INDEX_DIR/${hash}.content"
    if [[ -f "$f" ]]; then
        sqlite3 "$HASH_DB" "UPDATE hash_registry SET last_accessed=$(date +%s) WHERE hash='$hash';"
        cat "$f"
    else
        log_warn "Content not found for hash: $hash"
        return 1
    fi
}

# ---------------- BUILD SYSTEM ----------------
calculate_project_hash() {
    find "$PROJECT_DIR" -type f \( -name "*.js" -o -name "*.ts" -o -name "*.vue" -o -name "*.json" \) \
        -not -path "*/node_modules/*" -not -path "*/dist/*" \
        -exec sha256sum {} + | sort | sha256sum | cut -d' ' -f1
}

smart_vite_build() {
    local hash_file="$PROJECT_DIR/.build_hash"
    local new_hash=$(calculate_project_hash)
    local old_hash=""
    [[ -f "$hash_file" ]] && old_hash=$(cat "$hash_file")
    if [[ "$new_hash" == "$old_hash" ]]; then
        log_success "Build artifacts are current, skipping build"
        return 0
    fi
    log_info "Project changed, rebuilding..."
    vite_build || log_warn "Vite build failed"
    echo "$new_hash" > "$hash_file"
    store_hashed_content "BUILD:${new_hash}:$(date +%s)"
}

# ---------------- AI WORKFLOW STUBS ----------------
init_db() { log_info "init_db() called (stub)"; }
init_emoji_map() { log_info "init_emoji_map() called (stub)"; }

enhanced_cache_lookup() { return 1; }
run_agi_workflow() {
    local prompt="$*"
    log_info "run_agi_workflow() called with prompt: $prompt"
    conversation_history="[FINAL_ANSWER] Simulated response for: $prompt"
}
enhanced_store_memory() {
    local prompt="$1"
    local response="$2"
    store_hashed_content "PROMPT:${prompt}|RESPONSE:${response}|TIMESTAMP:$(date +%s)"
}
enhanced_ai_workflow() {
    local prompt="$*"
    local cached
    if cached=$(enhanced_cache_lookup "$prompt"); then
        log_success "Cache hit!"
        retrieve_hashed_content "$cached"
        return 0
    fi
    run_agi_workflow "$prompt"
    local answer
    answer=$(echo "$conversation_history" | grep '\[FINAL_ANSWER\]' | tail -n1 | sed 's/\[FINAL_ANSWER\]//')
    [[ -n "$answer" ]] && enhanced_store_memory "$prompt" "$answer"
}

# ---------------- CACHE MAINTENANCE ----------------
rehash_all() {
    local ts=$(date +%s)
    log_info "Rehashing all entries older than $REHASH_THRESHOLD seconds..."
    local hashes
    hashes=$(sqlite3 "$HASH_DB" "SELECT original_hash,timestamp FROM hash_registry;")
    while IFS='|' read -r orig ts_entry; do
        local diff=$((ts - ts_entry))
        if (( diff > REHASH_THRESHOLD )); then
            local content_file=$(sqlite3 "$HASH_DB" "SELECT content_ref FROM hash_registry WHERE original_hash='$orig';")
            if [[ -f "$content_file" ]]; then
                local content=$(cat "$content_file")
                store_hashed_content "$content"
            fi
        fi
    done <<< "$hashes"
    log_success "Rehash-all completed."
}

cleanup_cache() {
    local ts=$(date +%s)
    log_info "Cleaning up entries not accessed in $CACHE_CLEANUP_THRESHOLD seconds..."
    local old_entries
    old_entries=$(sqlite3 "$HASH_DB" "SELECT hash,content_ref,last_accessed FROM hash_registry;")
    while IFS='|' read -r hash content_file last_accessed; do
        local diff=$((ts - last_accessed))
        if (( diff > CACHE_CLEANUP_THRESHOLD )); then
            rm -f "$content_file"
            sqlite3 "$HASH_DB" "DELETE FROM hash_registry WHERE hash='$hash';"
            [[ "$VERBOSE" == true ]] && log_info "Removed stale hash: $hash"
        fi
    done <<< "$old_entries"
    log_success "Cleanup completed."
}

# ---------------- MAIN ----------------
main() {
    log_info "==== Starting Hash-Enhanced AI System ===="
    init_hash_system
    init_db
    init_emoji_map

    case "${1:-}" in
        "hash")
            shift
            store_hashed_content "$*"
            ;;
        "retrieve")
            retrieve_hashed_content "$2"
            ;;
        "index-status")
            sqlite3 "$HASH_DB" "SELECT COUNT(*),SUM(rehash_count),MAX(timestamp) FROM hash_registry;"
            ;;
        "build")
            smart_vite_build
            ;;
        "rehash-all")
            rehash_all
            ;;
        "cleanup-cache")
            cleanup_cache
            ;;
        *)
            enhanced_ai_workflow "$@"
            ;;
    esac
}

main "$@"

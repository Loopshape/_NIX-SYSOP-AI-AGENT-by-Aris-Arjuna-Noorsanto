#!/usr/bin/env bash\n# ai.sh - AI DevOps Platform v8.1 - Adaptive Triumvirate Mind Edition\n# Full single-file Bash implementation for advanced AI agent operations.\n\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# --- CONFIGURATION ---\nAI_HOME=\"${AI_HOME:-$HOME/.ai_agent}\"\nPROJECTS_DIR=\"${PROJECTS_DIR:-$HOME/ai_projects}\"\nSSH_DIR=\"$HOME/.ssh\"\nGIT_SSH_KEY=\"$SSH_DIR/id_ai_agent\"\n\n# Default Worker Models\nDEFAULT_MESSENGER_MODEL=\"gemma3:1b\"\nDEFAULT_COMBINATOR_MODEL=\"deepseek-r1:1.5b\"\nDEFAULT_TRADER_MODEL=\"2244-1\" # Mandatory executive model\n\n# Initialize with defaults - will be overridden by config\nMESSENGER_MODEL=\"\"\nCOMBINATOR_MODEL=\"\"\nTRADER_MODEL=\"\"\n\nOLLAMA_BIN=\"$(command -v ollama || true)\"\nMEMORY_DB=\"$AI_HOME/memory.db\"\nHASH_INDEX_DB=\"$AI_HOME/hashes.db\"\nPOOL_INDEX_DB=\"$AI_HOME/pool_index.db\"\nAPI_LOGS_DB=\"$AI_HOME/api_logs.db\"\nCONFIG_DB=\"$AI_HOME/config.db\"\n\nAPI_PORT=\"${API_PORT:-8080}\"\nAPI_PID_FILE=\"$AI_HOME/api.pid\"\nMAX_AGENT_ITERATIONS=10\nMAX_TRIUMVIRATE_ROUNDS=5\n\n# --- COLORS & LOGGING ---\nRED='\\033[0;31m'; GREEN='\\033[0;32m'; YELLOW='\\033[1;33m'; BLUE='\\033[0;34m'; PURPLE='\\033[0;35m'; CYAN='\\033[0;36m'; ORANGE='\\033[0;33m'; NC='\\033[0m'\nlog() { printf \"${BLUE}[%s]${NC} %s\\n\" \"$(date '+%T')\" \"$*\"; }\nlog_success() { log \"${GREEN}$*${NC}\"; }\nlog_warn() { log \"${YELLOW}WARN: $*${NC}\"; }\nlog_error() { log \"${RED}ERROR: $*${NC}\"; exit 1; }\nlog_info() { log \"${CYAN}$*${NC}\"; }\nlog_think() { printf \"${ORANGE}🤔 %s${NC}\\n\" \"$*\"; }\nlog_analysis() { printf \"${PURPLE}🔍 %s${NC}\\n\" \"$*\"; }\nlog_plan() { printf \"${CYAN}📋 %s${NC}\\n\" \"$*\"; }\nlog_execute() { printf \"${GREEN}⚡ %s${NC}\\n\" \"$*\"; }\nlog_memory() { printf \"${YELLOW}🧠 %s${NC}\\n\" \"$*\"; }\n\nlog_worker_start() {\n    echo -e \"\\n${PURPLE}┌────────────────────────────────────────────────────────────┐${NC}\"\n    echo -e \"${PURPLE}│ 🧠 WORKER: $1 ($2) ${NC}\"\n    echo -e \"${PURPLE}└────────────────────────────────────────────────────────────┘${NC}\"\n    echo -e \"${CYAN}--- AI Thinking (Live Stream) ---${NC}\"\n}\nlog_worker_end() {\n    echo -e \"${CYAN}--- End of Worker Thought ---${NC}\"\n    echo -e \"${PURPLE}════════════════════════════════════════════════════════════${NC}\\n\"\n}\n\n# --- BOOTSTRAP ---\nmkdir -p \"$AI_HOME\" \"$PROJECTS_DIR\" \"$SSH_DIR\"\nchmod 700 \"$SSH_DIR\" 2>/dev/null || true # Ensure permissions for .ssh\n\n# --- SQLite UTILITIES ---\nsqlite_escape() { echo \"$1\" | sed \"s/'/''/g\"; }\ninit_db() {\n    if ! command -v sqlite3 &> /dev/null; then\n        log_error \"sqlite3 is required. Install with: sudo apt-get install sqlite3 or brew install sqlite\"\n    fi\n    sqlite3 \"$MEMORY_DB\" \"CREATE TABLE IF NOT EXISTS memories (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP, prompt TEXT, response TEXT, pool_hash TEXT, task_hash TEXT);\" 2>/dev/null || true\n    sqlite3 \"$HASH_INDEX_DB\" \"CREATE TABLE IF NOT EXISTS hashes (type TEXT, target TEXT, hash TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP);\" 2>/dev/null || true\n    sqlite3 \"$CONFIG_DB\" \"CREATE TABLE IF NOT EXISTS config (key TEXT PRIMARY KEY, value TEXT);\" 2>/dev/null || true\n    sqlite3 \"$POOL_INDEX_DB\" \"CREATE TABLE IF NOT EXISTS pools (pool_hash TEXT PRIMARY KEY, rehash_count INTEGER DEFAULT 0, tasks TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP);\" 2>/dev/null || true\n    sqlite3 \"$API_LOGS_DB\" \"CREATE TABLE IF NOT EXISTS api_logs (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP, endpoint TEXT, method TEXT, payload TEXT);\" 2>/dev/null || true\n}\n\n# --- MEMORY MANAGEMENT ---\nadd_to_memory() { local p=\"$1\" r=\"$2\" ph=\"$3\" th=\"$4\"; sqlite3 \"$MEMORY_DB\" \"INSERT INTO memories (prompt, response, pool_hash, task_hash) VALUES ('$(sqlite_escape \"$p\")', '$(sqlite_escape \"$r\")', '$ph', '$th');\" 2>/dev/null; }\nsearch_memory() { local q=\"$1\" l=\"${2:-5}\"; sqlite3 -header -column \"$MEMORY_DB\" \"SELECT timestamp,prompt,response FROM memories WHERE prompt LIKE '%$(sqlite_escape \"$q\")%' OR response LIKE '%$(sqlite_escape \"$q\")%' ORDER BY timestamp DESC LIMIT $l;\" 2>/dev/null; }\nclear_memory() { if confirm_action \"Clear ALL memory data\"; then sqlite3 \"$MEMORY_DB\" \"DELETE FROM memories;\"; log_success \"Memory cleared.\"; else log_info \"Memory clear cancelled.\"; fi; }\n\n# --- HASHING ---\nhash_string() { echo -n \"$1\" | sha256sum | cut -d' ' -f1; }\nhash_file_content() { if [[ -f \"$1\" ]]; then sha256sum \"$1\" | cut -d' ' -f1; else echo \"ERROR: File not found: $1\"; return 1; fi; }\nhash_repo_content() { if [[ -d \"$1\" ]]; then find \"$1\" -type f ! -path \"*/.git/*\" -exec cat {} + 2>/dev/null | sha256sum | cut -d' ' -f1; else echo \"ERROR: Directory not found: $1\"; return 1; fi; }\n\nrecord_hash() { local type=\"$1\" target=\"$2\" hash=\"$3\"; sqlite3 \"$HASH_INDEX_DB\" \"INSERT OR REPLACE INTO hashes (type, target, hash) VALUES ('$type', '$(sqlite_escape \"$target\")', '$hash');\"; log_info \"Recorded hash for $type:$target -> $hash\"; }\nget_hash() { local type=\"$1\" target=\"$2\"; sqlite3 \"$HASH_INDEX_DB\" \"SELECT hash FROM hashes WHERE type='$type' AND target='$(sqlite_escape \"$target\")';\" 2>/dev/null; }\nview_hash_index() { sqlite3 -header -column \"$HASH_INDEX_DB\" \"SELECT * FROM hashes ORDER BY timestamp DESC;\" 2>/dev/null || echo \"No hashes recorded.\"; }\n\n# --- CONFIGURATION MANAGEMENT ---\nset_config() { local k=\"$1\" v=\"$2\"; sqlite3 \"$CONFIG_DB\" \"INSERT OR REPLACE INTO config (key, value) VALUES ('$k', '$(sqlite_escape \"$v\")');\"; log_success \"Config set: $k = $v\"; }\nget_config() { local k=\"$1\"; sqlite3 \"$CONFIG_DB\" \"SELECT value FROM config WHERE key = '$k';\" 2>/dev/null; }\nview_config() { sqlite3 -header -column \"$CONFIG_DB\" \"SELECT * FROM config;\" 2>/dev/null || echo \"No configuration set.\"; }\n\nload_config_values() {\n    # Use proper default value handling\n    local messenger_config=$(get_config messenger_model)\n    local combinator_config=$(get_config combinator_model)\n    local trader_config=$(get_config trader_model)\n    \n    MESSENGER_MODEL=\"${messenger_config:-$DEFAULT_MESSENGER_MODEL}\"\n    COMBINATOR_MODEL=\"${combinator_config:-$DEFAULT_COMBINATOR_MODEL}\"\n    TRADER_MODEL=\"${trader_config:-$DEFAULT_TRADER_MODEL}\"\n    \n    AI_TEMPERATURE=\"$(get_config temperature || echo \"0.7\")\"\n    AI_TOP_P=\"$(get_config top_p || echo \"0.9\")\"\n    AI_SEED=\"$(get_config seed || echo \"\")\"\n    API_PORT=\"$(get_config api_port || echo \"8080\")\"\n    \n    log_info \"Loaded models: Messenger=$MESSENGER_MODEL, Combinator=$COMBINATOR_MODEL, Trader=$TRADER_MODEL\"\n}\n\n# --- TASK POOLING ---\nsetup_task_pool() {\n    local prompt=\"$1\"\n    local semantic_hash_val\n    \n    log_think \"Analyzing prompt for semantic hashing...\"\n    # Simpler semantic hashing for bash-only. AI-based semantic hashing would require a model call here.\n    semantic_hash_val=$(echo "$prompt" | tr ' ' '-' | tr -cd 'a-zA-Z0-9-' | cut -c1-16)\n    if [[ -z "$semantic_hash_val" ]]; then\n        semantic_hash_val=$(hash_string "$prompt" | cut -c1-16)\n        log_warn \"Fallback: Generated semantic hash from raw prompt. AI-based semantic hashing would be more precise.\"\n    fi\n\n    local instance_hash_val=$(hash_string "$prompt$(date +%s%N)" | cut -c1-16)\n\n    local pool_dir=\"$PROJECTS_DIR/$semantic_hash_val\"\n    mkdir -p \"$pool_dir\"\n\n    local rehash_count=0\n    local tasks_json='[]'\n    local existing_data\n    existing_data=$(sqlite3 \"$POOL_INDEX_DB\" \"SELECT rehash_count, tasks FROM pools WHERE pool_hash = '$semantic_hash_val';\" 2>/dev/null)\n    \n    if [[ -n \"$existing_data\" ]]; then\n        rehash_count=$(echo \"$existing_data\" | cut -d'|' -f1)\n        local existing_tasks=$(echo \"$existing_data\" | cut -d'|' -f2)\n        rehash_count=$((rehash_count + 1))\n        if command -v jq &> /dev/null && [[ -n \"$existing_tasks\" ]]; then\n            tasks_json=$(echo \"$existing_tasks\" | jq -c --arg task \"$instance_hash_val\" '. + [$task]' 2>/dev/null || echo "[\"$instance_hash_val\"]")\n        else\n            tasks_json=\"[\\\"$instance_hash_val\\\"]\" # Fallback if jq not present or existing_tasks is empty\n        fi\n        sqlite3 \"$POOL_INDEX_DB\" \"UPDATE pools SET rehash_count = $rehash_count, tasks = '$(sqlite_escape \"$tasks_json\")' WHERE pool_hash = '$semantic_hash_val';\" 2>/dev/null\n    else\n        rehash_count=1\n        tasks_json=\"[\\\"$instance_hash_val\\\"]\"\n        sqlite3 \"$POOL_INDEX_DB\" \"INSERT INTO pools (pool_hash, rehash_count, tasks) VALUES ('$semantic_hash_val', $rehash_count, '$(sqlite_escape \"$tasks_json\")');\" 2>/dev/null\n    fi\n\n    log_memory \"Task pool created/updated: semantic=$semantic_hash_val, instance=$instance_hash_val, resonance=$rehash_count\"\n    echo \"$semantic_hash_val $instance_hash_val $rehash_count\"\n}\n\n# --- CORE AGENT TOOLS ---\nconfirm_action() {\n    local action=\"$1\"\n    echo -e \"${YELLOW}CONFIRM: $action${NC}\"\n    read -p \"Type 'yes' to confirm: \" -r response\n    if [[ \"$response\" == \"yes\" ]]; then\n        return 0\n    else\n        log_warn \"Action cancelled by user: $action\"\n        return 1\n    fi\n}\n\ntool_read_file() {\n    local p=\"$1\"\n    log_think \"Reading file: $p\"\n    if [[ -f \"$p\" ]]; then\n        cat \"$p\"\n    else\n        echo \"ERROR: File not found: $p\"\n    fi\n}\ntool_list_directory() {\n    local p=\"${1:-.}\"\n    log_think \"Listing directory: $p\"\n    if [[ -d \"$p\" ]]; then\n        if command -v tree &> /dev/null; then\n            tree -L 2 \"$p\"\n        else\n            ls -la \"$p\"\n        fi\n    else\n        echo \"ERROR: Directory not found: $p\"\n    fi\n}\ntool_web_search() {\n    if ! command -v googler &> /dev/null; then echo \"ERROR: googler not installed. Install with: sudo apt-get install googler\"; return 1; fi\n    local query=\"$1\"; local count=\"${2:-3}\"\n    log_think \"Searching web for: $query\"\n    if confirm_action \"Search web for: $query\"; then\n        googler --count \"$count\" --exact \"$query\"\n    else\n        echo \"ACTION CANCELED: Web search.\"\n    fi\n}\ntool_read_web_page() {\n    if ! command -v lynx &> /dev/null; then echo \"ERROR: lynx not installed. Install with: sudo apt-get install lynx\"; return 1; fi\n    local url=\"$1\"\n    log_think \"Reading web page: $url\"\n    if confirm_action \"Read web page: $url\"; then\n        curl -sL \"$url\" | lynx -dump -stdin\n    else\n        echo \"ACTION CANCELED: Read web page.\"\n    fi\n}\ntool_write_file() {\n    local path=\"$1\" content=\"$2\"\n    log_think \"Writing to file: $path\"\n    if confirm_action \"Write to file: $path\"; then\n        mkdir -p \"$(dirname \"$path\")\"\n        echo \"$content\" > \"$path\"\n        echo \"SUCCESS: File written to $path\"\n    else\n        echo \"ACTION CANCELED: Write to $path\"\n    fi\n}\ntool_create_directory() {\n    local path=\"$1\"\n    log_think \"Creating directory: $path\"\n    if confirm_action \"Create directory: $path\"; then\n        mkdir -p \"$path\"\n        echo \"SUCCESS: Directory created: $path\"\n    else\n        echo \"ACTION CANCELED: Create directory.\"\n    fi\n}\ntool_copy_file() {\n    local src=\"$1\" dest=\"$2\"\n    log_think \"Copying file: $src to $dest\"\n    if confirm_action \"Copy file: $src to $dest\"; then\n        cp -r \"$src\" \"$dest\"\n        echo \"SUCCESS: File copied: $src -> $dest\"\n    else\n        echo \"ACTION CANCELED: Copy file.\"\n    fi\n}\ntool_move_file() {\n    local src=\"$1\" dest=\"$2\"\n    log_think \"Moving file: $src to $dest\"\n    if confirm_action \"Move file: $src to $dest\"; then\n        mv \"$src\" \"$dest\"\n        echo \"SUCCESS: File moved: $src -> $dest\"\n    else\n        echo \"ACTION CANCELED: Move file.\"\n    fi\n}\ntool_delete_file() {\n    local path=\"$1\"\n    log_think \"Deleting file: $path\"\n    if confirm_action \"${RED}DELETE file: $path${NC}\"; then\n        rm -rf \"$path\"\n        echo \"SUCCESS: File deleted: $path\"\n    else\n        echo \"ACTION CANCELED: Delete file.\"\n    fi\n}\ntool_make_web_request() {\n    local url=\"$1\" method=\"${2:-GET}\" data=\"${3:-}\"\n    log_think \"Making $method request to: $url\"\n    if confirm_action \"Make $method request to: $url\"; then\n        local curl_cmd=\"curl -sL -X $method\"\n        if [[ -n \"$data\" ]]; then\n            curl_cmd+=\" -H 'Content-Type: application/json' -d '$data'\"\n        fi\n        eval \"$curl_cmd \\\"$url\\\"\"\n    else\n        echo \"ACTION CANCELED: Web request.\"\n    fi\n}\ntool_check_port() {\n    local host=\"$1\" port=\"$2\"\n    log_think \"Checking port $port on $host\"\n    if command -v nc &> /dev/null; then\n        if confirm_action \"Check port $port on $host\"; then\n            nc -zvw 1 \"$host\" \"$port\" 2>&1\n        else\n            echo \"ACTION CANCELED: Port check.\"\n        fi\n    else\n        echo \"ERROR: netcat (nc) not installed. Install with: sudo apt-get install netcat\"\n    fi\n}\ntool_download_file() {\n    local url=\"$1\" dest=\"$2\"\n    log_think \"Downloading: $url to $dest\"\n    if confirm_action \"Download: $url to $dest\"; then\n        if command -v wget &> /dev/null; then\n            wget -O \"$dest\" \"$url\"\n            echo \"SUCCESS: Downloaded to $dest\"\n        elif command -v curl &> /dev/null; then\n            curl -sL -o \"$dest\" \"$url\"\n            echo \"SUCCESS: Downloaded to $dest\"\n        else\n            echo \"ERROR: wget or curl not installed. Install one to download files.\"\n        fi\n    else\n        echo \"ACTION CANCELED: Download.\"\n    fi\n}\ntool_run_command() {\n    local cmd=\"$1\"\n    local project_root\n    project_root=$(get_config current_project_root || echo \".\") # Fallback to current dir\n    \n    log_think \"Running command: $cmd (in: $project_root)\"\n    if confirm_action \"Run command: $cmd (in: $project_root)\"; then\n        (cd \"$project_root\" && eval \"$cmd\" 2>&1)\n    else\n        echo \"ACTION CANCELED: Run command.\"\n    fi\n}\n\n# --- AI WORKER FUNCTIONS ---\n# This function directly interacts with Ollama via API to stream output\nrun_worker_raw() {\n    local worker_name=\"$1\" model_to_use=\"$2\" system_prompt=\"$3\" user_prompt=\"$4\"\n    \n    # Validate model parameter\n    if [[ -z \"$model_to_use\" ]]; then\n        log_error \"Model parameter is empty for worker: $worker_name\"\n    fi\n    \n    local temperature=\"${AI_TEMPERATURE:-0.7}\"\n    local top_p=\"${AI_TOP_P:-0.9}\"\n    local seed=\"${AI_SEED:-}\"\n    \n    local json_payload\n    json_payload=$(jq -n \\\n        --arg model \"$model_to_use\" \\\n        --arg system \"$system_prompt\" \\\n        --arg prompt \"$user_prompt\" \\\n        --argjson temp \"$temperature\" \\\n        --argjson top_p \"$top_p\" \\\n        '{ \"model\": $model, \"prompt\": $prompt, \"system\": $system, \"options\": { \"temperature\": $temp, \"top_p\": $top_p }, \"stream\": true }')\n    \n    if [[ -n \"$seed\" ]]; then\n        json_payload=$(echo \"$json_payload\" | jq --argjson seed \"$seed\" '.options.seed = $seed')\n    fi\n    \n    local response_buffer=\"\" # Buffer to capture the full response\n    \n    log_info \"Sending request to Ollama with model: $model_to_use\"\n    \n    local ollama_output\n    ollama_output=$(curl -s -X POST http://localhost:11434/api/generate \\\n        -H \"Content-Type: application/json\" \\\n        -d \"$json_payload\" | while IFS= read -r line; do\n        # Extract 'response' field, handle empty lines, and print to stdout\n        local extracted_response=$(echo \"$line\" | jq -r '.response? // empty' 2>/dev/null)\n        if [[ -n \"$extracted_response\" ]]; then\n            echo \"$extracted_response\"\n            response_buffer+=\"$extracted_response\"\n        fi\n    done)\n    \n    echo -e \"\\n${NC}\" # Newline to clean up after streaming\n    echo \"$response_buffer\"\n}\n\n# Orchestrates a worker's multi-turn tool-using loop\nrun_worker_interactive() {\n    local worker_name=\"$1\" model_to_use=\"$2\" system_prompt=\"$3\" user_prompt=\"$4\" context_memories=\"$5\" resonance_info=\"$6\" task_event_file=\"$7\"\n    \n    local full_system_prompt=\"$system_prompt\"\n    if [[ -n \"$context_memories\" ]]; then\n        full_system_prompt+=\"\\n\\nRELEVANT MEMORIES:\\n$context_memories\"\n        log_memory \"Injecting relevant memories into context\"\n    fi\n    if [[ -n \"$resonance_info\" ]]; then\n        full_system_prompt+=\"\\n\\nTASK RESONANCE: This task has been attempted $resonance_info times before. Learn from previous approaches.\"\n        log_memory \"Task resonance level: $resonance_info\"\n    }\n    \n    local current_conversation_history=\"$full_system_prompt\\n\\nUser Request: $user_prompt\"\n    local round_num=0\n\n    while [[ $round_num -lt $MAX_AGENT_ITERATIONS ]]; do\n        round_num=$((round_num + 1))\n        \n        log_info \"$worker_name Round $round_num/$MAX_AGENT_ITERATIONS...\"\n        log_worker_start \"$worker_name\" \"$model_to_use\"\n        \n        local ai_response=$(run_worker_raw \"$worker_name\" \"$model_to_use\" \"$current_conversation_history\")\n        local ai_response_json_escaped=$(printf '%s' \"$ai_response\" | jq -Rsa . 2>/dev/null || echo \"\\\"$(printf '%s' \"$ai_response\")\\\"\") # Escape for JSON\n        echo \"{\\\"worker\\\":\\\"$worker_name\\\",\\\"round\\\":$round_num,\\\"response\\\":$ai_response_json_escaped,\\\"timestamp\\\":\\\"$(date -Iseconds)\\\"}\" >> \"$task_event_file\"\n        \n        log_worker_end\n\n        # Check for [FINAL_ANSWER] to exit worker loop\n        if echo \"$ai_response\" | grep -q '^\\[FINAL_ANSWER\\]'; then\n            echo \"$ai_response\" # Pass the full response containing FINAL_ANSWER\n            return\n        fi\n\n        # Parse tools, handle multi-tool execution\n        local tool_results_block=\"\"\n        local tool_executed_in_round=false\n        \n        local tool_lines; IFS=$'\\n' read -r -d '' -a tool_lines <<< \"$(echo \"$ai_response\" | grep '^\\[TOOL\\]')\"\n        \n        if [[ ${#tool_lines[@]} -eq 0 ]]; then\n            log_warn \"$worker_name did not propose any tools. It might be stuck or ready to pass its current thinking.\" \n            # If no tools, pass current AI thinking to next round/worker\n            echo \"$ai_response\"\n            return\n        fi\n\n        for tool_line_raw in \"${tool_lines[@]}\"; do\n            local tool_name=$(echo \"$tool_line_raw\" | sed 's/^\\[TOOL\\] //g' | awk '{print $1}')\n            local tool_args=$(echo \"$tool_line_raw\" | sed 's/^\\[TOOL\\] //g' | cut -d' ' -f2-)\n            local result_output=\"\"\n\n            log_info \"$worker_name proposes tool: ${CYAN}$tool_name${NC} ${YELLOW}$tool_args${NC}\"\n            \n            case \"$tool_name\" in\n                read_file) result_output=$(tool_read_file \"$tool_args\") ;;\n                list_directory) result_output=$(tool_list_directory \"$tool_args\") ;;\n                web_search) result_output=$(tool_web_search \"$tool_args\") ;;\n                read_web_page) result_output=$(tool_read_web_page \"$tool_args\") ;;\n                write_file) \n                    local path=\"$tool_args\"\n                    local content_regex='^\\\[CONTENT\\\]$|^\\[/CONTENT\\]$' # Start and end tags\n                    local content=\"$(echo \"$ai_response\" | awk \"BEGIN{flag=0} /^\\[CONTENT\\]$/{flag=1;next} /^\\[\\/CONTENT\\]$/{flag=0} flag {print}\" )\"\n                    result_output=$(tool_write_file \"$path\" \"$content\")\n                    ;;\n                create_directory) result_output=$(tool_create_directory \"$tool_args\") ;;\n                copy_file) result_output=$(tool_copy_file \"$tool_args\") ;;\n                move_file) result_output=$(tool_move_file \"$tool_args\") ;;\n                delete_file) result_output=$(tool_delete_file \"$tool_args\") ;;\n                make_web_request) result_output=$(tool_make_web_request \"$tool_args\") ;;\n                check_port) result_output=$(tool_check_port \"$tool_args\") ;;\n                download_file) result_output=$(tool_download_file \"$tool_args\") ;;\n                run_command) result_output=$(tool_run_command \"$tool_args\") ;;\n                *) result_output=\"ERROR: Unknown tool '$tool_name' proposed by $worker_name.\" ;;\n            esac\n            tool_results_block+=\"\\n--- RESULT for $tool_name $tool_args ---\\n$result_output\"\n            tool_executed_in_round=true\n        done\n        \n        if $tool_executed_in_round; then\n            echo \"{\\\"worker\\\":\\\"$worker_name\\\",\\\"round\\\":$round_num,\\\"tool_results\\\": $(printf '%s' \"$tool_results_block\" | jq -Rsa . 2>/dev/null || echo \"\\\"$(printf '%s' \"$tool_results_block\")\\\"\"),\\\"timestamp\\\":\\\"$(date -Iseconds)\\\"}\" >> \"$task_event_file\"\n            current_conversation_history+=\"\\n$ai_response\\n[TOOL_RESULTS]\\n$tool_results_block\"\n        else\n            log_warn \"No tools were successfully executed in this round by $worker_name (user cancellation or errors). Passing current AI thought to next round.\"\n            current_conversation_history+=\"\\n$ai_response\"\n        fi\n    done\n\n    echo \"$ai_response\" # Should not be reached if FINAL_ANSWER is handled correctly\n}\n\n# --- TRIUMVIRATE AGENT ORCHESTRATION ---\nrun_triumvirate_agent() {\n    local user_prompt=\"$1\" semantic_hash=\"$2\" instance_hash=\"$3\" rehash_count=\"$4\"\n    \n    local project_dir=\"$PROJECTS_DIR/$semantic_hash/$instance_hash\"\n    mkdir -p \"$project_dir\"\n    set_config \"current_project_root\" \"$project_dir\" # Store project root for commands\n    local task_event_file=\"$project_dir/events.jsonl\"\n\n    echo -e \"\\n${GREEN}🚀 STARTING TRIUMVIRATE MIND PROCESSING FOR: ${CYAN}$user_prompt${NC}\"\n    log_info \"Project directory: $project_dir\"\n    log_info \"Semantic hash: $semantic_hash\"\n    log_info \"Instance hash: $instance_hash\"\n    log_info \"Resonance count: $rehash_count\"\n\n    # Phase 0: Search for relevant memories\n    log_think \"Searching memory for relevant context...\"\n    local context_memories; context_memories=$(search_memory \"$user_prompt\" 3 2>/dev/null || echo \"No relevant memories found.\")\n\n    local final_trader_response=\"\"\n    for ((tri_round=1; tri_round<=MAX_TRIUMVIRATE_ROUNDS; tri_round++)); do\n        echo -e \"\\n${CYAN}---------- TRIUMVIRATE ROUND $tri_round/$MAX_TRIUMVIRATE_ROUNDS ----------${NC}\"\n\n        # Phase 1: Messenger (Understanding and Analysis)\n        log_analysis \"Messenger Phase: Analyzing user request and gathering initial data...\"\n        local messenger_system_prompt=\"You are the MESSENGER. Your role is to analyze the user's request, break it down into core components, and use tools to gather initial data.\nAvailable tools: read_file, list_directory, web_search, read_web_page.\nOutput format: THOUGHT: [your analysis]\\n[TOOL] [tool_name] [parameters] (can be multiple) or FINAL: [summary of initial findings]\"\n        local messenger_response=$(run_worker_interactive \"Messenger\" \"$MESSENGER_MODEL\" \"$messenger_system_prompt\" \"$user_prompt\" \"$context_memories\" \"$rehash_count\" \"$task_event_file\")\n        \n        if echo \"$messenger_response\" | grep -q 'FINAL:'; then\n            log_warn \"Messenger provided a FINAL response. Skipping Combinator/Trader for this round.\"\n            final_trader_response=\"Messenger's Final Findings: $(echo \"$messenger_response\" | grep 'FINAL:' | sed 's/FINAL: //')\"\n            break # Exit triumvirate rounds\n        fi\n        \n        # Phase 2: Combinator (Planning and Strategy)\n        log_plan \"Combinator Phase: Creating execution plan...\"\n        local combinator_system_prompt=\"You are the COMBINATOR. Based on the user's request and the Messenger's report, create a detailed, step-by-step execution plan using the available tools.\nAvailable tools: ALL tools (read_file, list_directory, web_search, read_web_page, write_file, create_directory, copy_file, move_file, delete_file, make_web_request, check_port, download_file, run_command).\nOutput format: THOUGHT: [your plan]\\n[TOOL] [tool_name] [parameters] (can be multiple) or FINAL: [execution_plan summary]\"\n        local combinator_user_prompt=\"User Request: ${user_prompt}\\nMessenger's Report:\\n${messenger_response}\"\n        local combinator_response=$(run_worker_interactive \"Combinator\" \"$COMBINATOR_MODEL\" \"$combinator_system_prompt\" \"$combinator_user_prompt\" \"$context_memories\" \"$rehash_count\" \"$task_event_file\")\n\n        if echo \"$combinator_response\" | grep -q 'FINAL:'; then\n            log_warn \"Combinator provided a FINAL response. Skipping Trader for this round.\"\n            final_trader_response=\"Combinator's Final Plan: $(echo \"$combinator_response\" | grep 'FINAL:' | sed 's/FINAL: //')\"\n            break # Exit triumvirate rounds\n        fi\n\n        # Phase 3: Trader (Execution and Decision)\n        log_execute \"Trader Phase: Executing plan and making decisions...\"\n        local trader_system_prompt=\"You are the TRADER. Based on the user's original request, the Messenger's report, and the Combinator's plan, execute the necessary actions using ALL available tools. Provide a [FINAL_ANSWER] once the task is complete.\"\n        local trader_user_prompt=\"Original Request: ${user_prompt}\\nMessenger's Report:\\n${messenger_response}\\nCombinator's Plan:\\n${combinator_response}\"\n        local trader_response=$(run_worker_interactive \"Trader\" \"$TRADER_MODEL\" \"$trader_system_prompt\" \"$trader_user_prompt\" \"$context_memories\" \"$rehash_count\" \"$task_event_file\")\n\n        if echo \"$trader_response\" | grep -q '^\\[FINAL_ANSWER\\]'; then\n            final_trader_response=\"$trader_response\"\n            break # Exit triumvirate rounds\n        else\n            log_warn \"Trader did not provide FINAL_ANSWER. This round's full output will be used as context for the next Triumvirate round.\"\n        fi\n    done\n\n    if [[ -z \"$final_trader_response\" ]]; then\n        final_trader_response=\"ERROR: Maximum Triumvirate rounds ($MAX_TRIUMVIRATE_ROUNDS) reached without a [FINAL_ANSWER].\" \n        log_error \"Max Triumvirate rounds reached without final answer.\"\n    fi\n\n    # Store final interaction in memory\n    log_memory \"Storing final result in memory...\"\n    add_to_memory \"$user_prompt\" \"$final_trader_response\" \"$semantic_hash\" \"$instance_hash\"\n\n    echo -e \"\\n${GREEN}✅ === TRIUMVIRATE PROCESS COMPLETE ===${NC}\"\n    echo -e \"${GREEN}📝 Final Response:${NC}\"\n    # Extract content after [FINAL_ANSWER] tag if present\n    echo \"$final_trader_response\" | sed -n '/^\\[FINAL_ANSWER\\]/,$p' | sed '1d'\n    \n    # Save full response to task directory\n    local task_dir=\"$PROJECTS_DIR/$semantic_hash/$instance_hash\"\n    echo \"User Prompt: $user_prompt\" > \"$task_dir/summary.txt\"\n    echo -e \"\\n--- Final Agent Response ---\\n\" >> \"$task_dir/summary.txt\"\n    echo \"$final_trader_response\" >> \"$task_dir/summary.txt\"\n    log_success \"Full task log and summary saved in: $task_dir\"\n}\n\n# --- API SERVER ---\nstart_api_server() {\n    if [[ -f \"$API_PID_FILE\" ]] && kill -0 \"$(cat \"$API_PID_FILE\")\" 2>/dev/null; then\n        log_info \"API server already running (PID: $(cat \"$API_PID_FILE\"))\"\n        return 0\n    fi\n    \n    log_info \"Starting API server on port $API_PORT...\"\n    \n    # This is a simplified netcat server. A robust production API would use Node.js/Python.\n    (while true; do \n        { \n            read -r request_line; \n            local method=$(echo \"$request_line\" | awk '{print $1}'); \n            local path=$(echo \"$request_line\" | awk '{print $2}');\n            \n            local response_body=\"\"\n            local http_status=\"200 OK\"\n            local content_type=\"application/json\"\n            \n            case \"$method $path\" in\n                \"GET /status\")\n                    response_body=\"{\\\"status\\\": \\\"running\\\", \\\"version\\\": \\\"v8.1\\\", \\\"timestamp\\\": \\\"$(date -Iseconds)\\\", \\\"model\\\": \\\"$(get_config trader_model || echo 'default')\\\"}\"\n                    sqlite3 \"$API_LOGS_DB\" \"INSERT INTO api_logs (endpoint, method, payload) VALUES ('/status', 'GET', 'N/A');\" 2>/dev/null\n                    ;;\n                \"POST /prompt\")\n                    local content_length=0\n                    while read -r header && [[ \"$header\" != $'\r' ]]; do\n                        if [[ \"$header\" == Content-Length* ]]; then content_length=${header#*Content-Length: }; fi\n                    done\n                    local body=\"\"\n                    if [[ \"$content_length\" -gt 0 ]]; then\n                        read -r -n \"$content_length\" body\n                    fi\n                    \n                    local prompt_payload=\"$(echo \"$body\" | jq -r '.prompt' 2>/dev/null)\"\n                    \n                    if [[ -n \"$prompt_payload\" ]]; then\n                        sqlite3 \"$API_LOGS_DB\" \"INSERT INTO api_logs (endpoint, method, payload) VALUES ('/prompt', 'POST', '$(sqlite_escape \"$prompt_payload\")');\" 2>/dev/null\n                        \n                        # Run agent in background, detach from netcat process\n                        (run_triumvirate_agent \"$prompt_payload\" >/dev/null 2>&1 &) \n                        local agent_pid=$! # Get PID of the backgrounded agent\n                        response_body=\"{\\\"status\\\": \\\"accepted\\\", \\\"message\\\": \\\"Agent started in background (PID: $agent_pid). Check logs for details.\\\", \\\"prompt\\\": $(printf '%s' \"$prompt_payload\" | jq -Rsa .)}\"\n                    else\n                        http_status=\"400 Bad Request\"\n                        response_body=\"{\\\"error\\\": \\\"Invalid JSON or missing prompt\\\"}\"\n                    fi\n                    ;;\n                *)\n                    http_status=\"404 Not Found\"\n                    response_body=\"{\\\"error\\\": \\\"Endpoint not found\\\"}\"\n                    ;;\n            esac\n            echo -e \"HTTP/1.1 $http_status\\r\\nContent-Type: $content_type\\r\\nContent-Length: \${#response_body}\\r\\n\\r\\n$response_body\"\n        } | nc -l -p \"$API_PORT\" -q 1\n    done) & \n    \n    local api_pid=$! \n    echo \"$api_pid\" > \"$API_PID_FILE\"\n    log_success \"API server started (PID: $api_pid) on http://localhost:$API_PORT\"\n}\n\nstop_api_server() {\n    if [[ -f \"$API_PID_FILE\" ]]; then \n        local pid=$(cat \"$API_PID_FILE\")\n        if kill -0 \"$pid\" 2>/dev/null; then\n            log_info \"Stopping API server PID $pid...\"\n            kill \"$pid\"\n            # Also kill any backgrounded agent processes that might be children of the server\n            pkill -P \"$pid\" \"bash\" 2>/dev/null || true # Best effort to kill child bash processes\n            rm -f \"$API_PID_FILE\"\n            log_success \"API server stopped\"\n        else\n            log_warn \"No API server process found for PID $pid. Removing stale PID file.\"\n            rm -f \"$API_PID_FILE\"\n        fi\n    else\n        log_warn \"No API server PID file found\"\n    fi\n}\n\napi_status() {\n    if [[ -f \"$API_PID_FILE\" ]] && kill -0 \"$(cat \"$API_PID_FILE\")\" 2>/dev/null; then\n        log_success \"API server is running (PID: $(cat \"$API_PID_FILE\"))\"\n        return 0\n    else\n        log_warn \"API server is not running\"\n        return 1\n    fi\n}\n\n# --- DEPENDENCY CHECK & SETUP ---\ncheck_dependencies() {\n    local missing=()\n    local optional_warns=()\n    \n    for cmd in sqlite3 jq curl git nc; do # googler and lynx are checked in their tools\n        if ! command -v \"$cmd\" &> /dev/null; then\n            missing+=(\"$cmd\")\n        fi\n    done\n    \n    if ! command -v \"$OLLAMA_BIN\" &> /dev/null; then\n        log_warn \"Ollama not found. Install from https://ollama.ai\"\n        missing+=(\"ollama\")\n    else\n        # Check if models are available\n        local available_models\n        available_models=$(ollama list 2>/dev/null || echo \"\")\n        \n        for model in \"$MESSENGER_MODEL\" \"$COMBINATOR_MODEL\" \"$TRADER_MODEL\"; do\n            if ! echo \"$available_models\" | grep -q \"$model\"; then\n                log_warn \"Model $model not found. Run: ollama pull $model\"\n                optional_warns+=(\"Ollama model $model\")\n            fi\n        done\n    fi\n    \n    if [[ ${#missing[@]} -gt 0 ]]; then\n        log_error \"Missing critical dependencies. Please install: ${missing[*]}\"\n        return 1\n    fi\n    \n    if [[ ${#optional_warns[@]} -gt 0 ]]; then\n        log_warn \"Optional components not found: ${optional_warns[*]}\"\n    fi\n    log_success \"Core dependencies satisfied\"\n    return 0\n}\n\nsetup_environment() {\n    log_info \"Setting up AI Agent environment...\"\n    init_db # Ensure DBs are initialized\n    \n    # Set default config values if not present\n    if [[ -z \"$(get_config messenger_model)\" ]]; then set_config messenger_model \"$DEFAULT_MESSENGER_MODEL\"; fi\n    if [[ -z \"$(get_config combinator_model)\" ]]; then set_config combinator_model \"$DEFAULT_COMBINATOR_MODEL\"; fi\n    if [[ -z \"$(get_config trader_model)\" ]]; then set_config trader_model \"$DEFAULT_TRADER_MODEL\"; fi\n    if [[ -z \"$(get_config temperature)\" ]]; then set_config temperature \"0.7\"; fi\n    if [[ -z \"$(get_config top_p)\" ]]; then set_config top_p \"0.9\"; fi\n    if [[ -z \"$(get_config api_port)\" ]]; then set_config api_port \"$API_PORT\"; fi\n    \n    # Setup default SSH config if missing\n    if [[ ! -f \"$GIT_SSH_KEY\" ]]; then\n        log_warn \"No SSH key found at $GIT_SSH_KEY. Git operations may require manual setup.\"\n    fi\n    if [[ ! -f \"$SSH_DIR/config\" ]]; then\n        cat > \"$SSH_DIR/config\" <<EOF\nHost github.com\\n    HostName github.com\\n    User git\\n    IdentityFile $GIT_SSH_KEY\\n    IdentitiesOnly yes\\n\\nHost gitlab.com\\n    HostName gitlab.com\\n    User git\\n    IdentityFile $GIT_SSH_KEY\\n    IdentitiesOnly yes\\n\\nHost bitbucket.org\\n    HostName bitbucket.org\\n    User git\\n    IdentityFile $GIT_SSH_KEY\\n    IdentitiesOnly yes\nEOF\n        chmod 600 \"$SSH_DIR/config\" 2>/dev/null || true\n        log_info \"Created default SSH config.\"\n    fi\n\n    load_config_values # Load these values into script vars\n    log_success \"Environment setup complete\"\n}\n\n# --- CLI Dispatcher ---\nmain() {\n    # Initialize DBs and load config at start of main() for all commands\n    init_db\n    load_config_values\n    \n    # Ensure ollama is callable before any model interaction\n    if ! command -v \"$OLLAMA_BIN\" &> /dev/null; then\n        log_error \"Ollama executable not found at '$OLLAMA_BIN'. Please install Ollama or correct OLLAMA_BIN path.\"\n    fi\n\n    case \"${1:-}\" in\n        \"--setup\")\n            check_dependencies\n            setup_environment\n            ;;\n        \"--config\")\n            case \"${2:-}\" in\n                \"set\") \n                    if [[ -z \"$3\" || -z \"$4\" ]]; then log_error \"Usage: $0 --config set <key> <value>\"; fi\n                    set_config \"$3\" \"$4\" \n                    ;;\n                \"get\") \n                    if [[ -z \"$3\" ]]; then log_error \"Usage: $0 --config get <key>\"; fi\n                    get_config \"$3\" \n                    ;;\n                \"view\") view_config ;;\n                *) log_error \"Usage: $0 --config [set|get|view] [key] [value]\";; \n            esac\n            ;;\n        \"--hash\")\n            case \"${2:-}\" in\n                \"file\") \n                    if [[ -z \"$3\" ]]; then log_error \"Usage: $0 --hash file <path>\"; fi\n                    local file_hash=$(hash_file_content \"$3\")\n                    record_hash \"file\" \"$3\" \"$file_hash\" \n                    ;;\n                \"prompt\") \n                    if [[ -z \"$3\" ]]; then log_error \"Usage: $0 --hash prompt \\\"<text>\\\"\"; fi\n                    local prompt_hash=$(hash_string \"$3\")\n                    record_hash \"prompt\" \"$3\" \"$prompt_hash\" \n                    ;;\n                \"repo\") \n                    local repo_path=\"${3:-.}\"\n                    local repo_hash=$(hash_repo_content \"$repo_path\")\n                    record_hash \"repo\" \"$repo_path\" \"$repo_hash\" \n                    ;;\n                \"get\") \n                    if [[ -z \"$3\" || -z \"$4\" ]]; then log_error \"Usage: $0 --hash get <type> <target>\"; fi\n                    get_hash \"$3\" \"$4\" \n                    ;;\n                \"view\") view_hash_index ;;\n                *) log_error \"Usage: $0 --hash [file|prompt|repo|get|view] [target/query] [value]\";; \n            esac\n            ;;\n        \"--memory\")\n            case \"${2:-}\" in\n                \"search\") \n                    if [[ -z \"$3\" ]]; then log_error \"Usage: $0 --memory search \\\"<query>\\\"\"; fi\n                    search_memory \"$3\" \"${4:-5}\" \n                    ;;\n                \"clear\") clear_memory ;;\n                *) log_error \"Usage: $0 --memory [search|clear] [query] [limit]\";; \n            esac\n            ;;\n        \"--api\")\n            case \"${2:-}\" in\n                \"start\") start_api_server ;;\n                \"stop\") stop_api_server ;;\n                \"status\") api_status ;;\n                *) log_error \"Usage: $0 --api [start|stop|status]\";; \n            esac\n            ;;\n        \"--tools\")\n            echo -e \"${CYAN}Available Agent Tools:${NC}\"\n            echo -e \"  read_file <path>\"\n            echo -e \"  list_directory <path>\"\n            echo -e \"  web_search <query> [count]\"\n            echo -e \"  read_web_page <url>\"\n            echo -e \"  write_file <path> <content>\"\n            echo -e \"  create_directory <path>\"\n            echo -e \"  copy_file <source> <destination>\"\n            echo -e \"  move_file <source> <destination>\"\n            echo -e \"  delete_file <path>\"\n            echo -e \"  make_web_request <url> [method] [data]\"\n            echo -e \"  check_port <host> <port>\"\n            echo -e \"  download_file <url> <destination_path>\"\n            echo -e \"  run_command <command>\"\n            ;;\n        \"--help\"|\"-h\"|\"\")\n            cat << EOF\n${GREEN}AI DevOps Platform v8.1 - Adaptive Triumvirate Mind Edition${NC}\n\n${CYAN}Description:${NC} A multi-worker AI agent for complex tasks, learning, and system management.\n\n${CYAN}Usage:${NC}\n  $0 [OPTIONS] \"<your prompt>\"      - Run the Triumvirate Agent\n  $0 --help                        - Show this help message\n\n${CYAN}Commands:${NC}\n  $0 --setup\n  $0 --config [set|get|view] [key] [value]\n  $0 --hash [file|prompt|repo|get|view] [target/query] [hash]\n  $0 --memory [search|clear] [query] [limit]\n  $0 --api [start|stop|status]\n  $0 --tools\n\n${CYAN}Examples:${NC}\n  $0 \"Setup a new Node.js project and install Express.\"\n  $0 --config set trader_model llama3.1:8b\n  $0 --memory search \"Node.js project\"\n  $0 --api start\n  $0 --hash file ./ai.sh\nEOF\n            ;;\n        *) # Default: Run Triumvirate Agent\n            local prompt_to_run=\"$@\"\n            if [[ -z \"$prompt_to_run\" ]]; then\n                log_error \"No prompt provided. Use '$0 --help' for usage.\"\n            fi\n            check_dependencies\n            run_triumvirate_agent \"$prompt_to_run\"\n            ;;\n    esac\n}\n\n# --- TRAP FOR CLEANUP ---\ntrap 'log_warn \"Script interrupted or exited. Cleaning up API server if running...\"; stop_api_server; exit 0' INT TERM EXIT\n\n# --- ENTRY POINT ---\nif [[ \"${BASH_SOURCE[0]}\" == \"${0}\" ]]; then\n    main \"$@\"\nfi\n"

#!/usr/bin/env bash
# ai.sh - AI Hybrid Agent v22 â€“ Turbo, Weighted Fuzzy Cache, Smart Swap, Fast SQLite

set -euo pipefail
IFS=$'\n\t'

# ---------------- CONFIG ----------------
SCRIPT_NAME="ai"
SCRIPT_VERSION="22.0"
AI_HOME="${AI_HOME:-$HOME/.ai_agent}"
PROJECTS_DIR="${PROJECTS_DIR:-$HOME/ai_projects}"
LOG_FILE="$AI_HOME/ai.log"
LOG_LEVEL="${LOG_LEVEL:-INFO}" # Set to DEBUG for more verbosity

DEFAULT_MESSENGER_MODEL="gemma:2b"
DEFAULT_COMBINATOR_MODEL="llama3:8b"
DEFAULT_TRADER_MODEL="gemma2:9b"

MESSENGER_MODEL="${MESSENGER_MODEL:-$DEFAULT_MESSENGER_MODEL}"
COMBINATOR_MODEL="${COMBINATOR_MODEL:-$DEFAULT_COMBINATOR_MODEL}"
TRADER_MODEL="${TRADER_MODEL:-$DEFAULT_TRADER_MODEL}"

OLLAMA_BIN="$(command -v ollama || true)"

MEMORY_DB="$AI_HOME/memory.db"
HASH_INDEX_DB="$AI_HOME/hashes.db"
POOL_INDEX_DB="$AI_HOME/pool_index.db"
CONFIG_DB="$AI_HOME/config.db"
API_LOGS_DB="$AI_HOME/api_logs.db"

MAX_TRIUMVIRATE_ROUNDS=3
MAX_RAM_BYTES=2097152 # 2MB threshold for swapping to disk
SWAP_DIR="$AI_HOME/swap"

mkdir -p "$AI_HOME" "$PROJECTS_DIR" "$SWAP_DIR"

# ---------------- COLORS ----------------
RED='\033[0;31m'; GREEN='\033[0;32m'; YELLOW='\033[1;33m'
BLUE='\033[0;34m'; PURPLE='\033[0;35m'; CYAN='\033[0;36m'
ORANGE='\033[0;33m'; NC='\033[0m'

# ---------------- LOGGING ----------------
log_to_file(){ echo "[$(date '+%Y-%m-%d %H:%M:%S')] [$1] $2" >> "$LOG_FILE"; }
log_debug(){ [[ "$LOG_LEVEL" == "DEBUG" ]] && printf "${PURPLE}[DEBUG][%s]${NC} %s\n" "$(date '+%T')" "$*" >&2 && log_to_file "DEBUG" "$*"; }
log_info(){ [[ "$LOG_LEVEL" =~ ^(DEBUG|INFO)$ ]] && printf "${BLUE}[INFO][%s]${NC} %s\n" "$(date '+%T')" "$*" >&2 && log_to_file "INFO" "$*"; }
log_warn(){ printf "${YELLOW}[WARN][%s]${NC} %s\n" "$(date '+%T')" "$*" >&2 && log_to_file "WARN" "$*"; }
log_error(){ printf "${RED}[ERROR][%s]${NC} %s\n" "$(date '+%T')" "$*" >&2 && log_to_file "ERROR" "$*" && return 1; }
log_success(){ printf "${GREEN}[SUCCESS][%s]${NC} %s\n" "$(date '+%T')" "$*" >&2 && log_to_file "SUCCESS" "$*"; }
log_think(){ printf "${ORANGE}ðŸ¤” [THINK][%s]${NC} %s\n" "$(date '+%T')" "$*" >&2 && log_to_file "THINK" "$*"; }
log_plan(){ printf "${CYAN}ðŸ“‹ [PLAN][%s]${NC} %s\n" "$(date '+%T')" "$*" >&2 && log_to_file "PLAN" "$*"; }
log_execute(){ printf "${GREEN}âš¡ [EXECUTE][%s]${NC} %s\n" "$(date '+%T')" "$*" >&2 && log_to_file "EXECUTE" "$*"; }

# ---------------- DATABASE ----------------
sqlite_escape(){ echo "$1" | sed "s/'/''/g"; }

init_db(){
  local dbs=("$MEMORY_DB" "$HASH_INDEX_DB" "$CONFIG_DB" "$POOL_INDEX_DB" "$API_LOGS_DB")
  local schemas=(
    "CREATE TABLE IF NOT EXISTS memories (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP, prompt TEXT, response TEXT, prompt_hash TEXT, task_hash TEXT); CREATE INDEX IF NOT EXISTS idx_prompt_hash ON memories(prompt_hash);"
    "CREATE TABLE IF NOT EXISTS hashes (type TEXT, target TEXT, hash TEXT PRIMARY KEY, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP);"
    "CREATE TABLE IF NOT EXISTS config (key TEXT PRIMARY KEY, value TEXT);"
    "CREATE TABLE IF NOT EXISTS pools (pool_hash TEXT PRIMARY KEY, rehash_count INTEGER DEFAULT 0, tasks TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP);"
    "CREATE TABLE IF NOT EXISTS api_logs (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP, endpoint TEXT, method TEXT, payload TEXT);"
  )
  for i in "${!dbs[@]}"; do
      sqlite3 "${dbs[$i]}" "${schemas[$i]}" 2>/dev/null || log_warn "DB init failed: ${dbs[$i]}"
  done
  log_success "Databases initialized"
}

add_to_memory_fast(){
  local prompt="$1" response_ref="$2" prompt_hash="$3" task_hash="$4"
  sqlite3 "$MEMORY_DB" <<SQL
PRAGMA journal_mode=WAL;
PRAGMA synchronous=NORMAL;
INSERT INTO memories (prompt,response,prompt_hash,task_hash)
VALUES ('$(sqlite_escape "$prompt")','$(sqlite_escape "$response_ref")','$(sqlite_escape "$prompt_hash")','$(sqlite_escape "$task_hash")');
SQL
  log_debug "Memory added for prompt hash: $prompt_hash"
}

# ---------------- HASH & SWAP ----------------
hash_string(){ echo -n "$1" | sha256sum | cut -d' ' -f1; }

store_output_fast(){
    local content="$1"
    local size_bytes=${#content}
    local hash=$(hash_string "$content")
    
    if (( size_bytes > MAX_RAM_BYTES )); then
        local file_path="$SWAP_DIR/$hash.txt.gz"
        echo "$content" | gzip > "$file_path"
        log_info "Output too large (${size_bytes}B), offloaded to disk: $file_path"
        echo "$file_path" # Return the file path as the reference
    else
        echo "$content" # Return the content itself as the reference
    fi
}

retrieve_output_fast(){
    local ref="$1"
    if [[ -f "$ref" ]]; then # Check if the reference is a file path
        if [[ "$ref" == *.gz ]]; then
            gzip -dc "$ref"
        else
            cat "$ref"
        fi
    else
        echo "$ref" # It's not a file path, so it's the content itself
    fi
}

# ---------------- TASK POOL ----------------
setup_task_pool(){
    local prompt="$1"
    local prompt_hash=$(semantic_hash_prompt "$prompt")
    local instance_hash=$(hash_string "$prompt$(date +%s%N)" | cut -c1-16)
    local task_dir="$PROJECTS_DIR/$prompt_hash"
    mkdir -p "$task_dir"
    log_info "Task workspace: $task_dir"
    echo "$prompt_hash $instance_hash"
}

# ---------------- WORKERS ----------------
run_worker_fast(){
    local model="$1" system_context="$2" prompt="$3"
    
    # Use jq to safely create the JSON payload
    local payload
    payload=$(jq -nc \
        --arg model "$model" \
        --arg system "$system_context" \
        --arg prompt "$prompt" \
        '{model: $model, system: $system, prompt: $prompt, options: {temperature: 0.7, top_p: 0.9}, stream: false}')
    
    # Log the API call
    log_to_file "API_CALL" "Model: $model, Payload: $payload"

    # Make the API call and extract the 'response' field from the JSON output
    local response_json
    response_json=$(curl -s --max-time 300 -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d "$payload")
    
    if ! jq -e . >/dev/null 2>&1 <<<"$response_json"; then
        log_error "Received invalid JSON from Ollama API: $response_json"
        echo "Ollama API Error: Invalid JSON response."
        return
    fi
    
    # Check for an error message in the JSON response
    local error_message
    error_message=$(echo "$response_json" | jq -r '.error // empty')
    if [[ -n "$error_message" ]]; then
        log_error "Ollama API returned an error: $error_message"
        echo "Ollama API Error: $error_message"
        return
    fi
    
    # Extract and return the content of the 'response' field
    echo "$response_json" | jq -r '.response // empty'
}

# ---------------- LEVENSHTEIN ----------------
levenshtein(){
    awk -v s1="$1" -v s2="$2" '
    BEGIN{
        n=length(s1); m=length(s2)
        for(i=0;i<=n;i++) d[i,0]=i
        for(j=0;j<=m;j++) d[0,j]=j
        for(i=1;i<=n;i++){
            for(j=1;j<=m;j++){
                cost=(substr(s1,i,1)==substr(s2,j,1)?0:1)
                d[i,j]=min(min(d[i-1,j]+1,d[i,j-1]+1),d[i-1,j-1]+cost)
            }
        }
        print d[n,m]
    }
    function min(a,b,c){ if(c=="") c=99999; if(a<=b && a<=c) return a; else if(b<=a && b<=c) return b; else return c }'
}

semantic_hash_prompt(){ echo "$1" | tr '[:upper:]' '[:lower:]' | tr -cs 'a-z0-9' ' ' | tr -s ' ' | sed 's/ ^*//;s/ *$//' | tr ' ' '_'; }

# ---------------- WEIGHTED FUZZY CACHE ----------------
get_weighted_cached_response(){
    local prompt="$1"
    local threshold="${2:-15}" # Levenshtein distance threshold
    local w_sim=0.6 w_rec=0.3 w_size=0.1 # Weights for Similarity, Recency, Size

    local prompt_hash=$(semantic_hash_prompt "$prompt")
    local best_score=-1.0
    local best_response=""

    log_debug "Fuzzy Cache: Searching for prompt hash '$prompt_hash' with threshold $threshold"

    # Fetch all records to process in bash. For huge DBs, this could be slow, but it's fine for thousands of entries.
    local memories
    memories=$(sqlite3 "$MEMORY_DB" "SELECT prompt_hash, response, timestamp FROM memories;")

    while IFS='|' read -r db_prompt_hash response_ref timestamp; do
        local dist score recency_score size_score sim

        dist=$(levenshtein "$prompt_hash" "$db_prompt_hash")
        
        if (( dist > threshold )); then
            continue
        fi

        # Similarity Score (inverted distance)
        sim=$(awk -v d="$dist" -v t="$threshold" 'BEGIN{printf "%0.6f", 1-(d/t)}')

        # Recency Score (decays over time)
        ts_epoch=$(date -d "$timestamp" +%s 2>/dev/null || echo 0)
        now_epoch=$(date +%s)
        recency_score=$(awk -v now="$now_epoch" -v ts="$ts_epoch" 'BEGIN{printf "%0.6f", 1/((now-ts)/86400 + 1)}')

        # Size Score (penalizes large differences in prompt length)
        size_score=$(awk -v ps=${#prompt_hash} -v rs=${#db_prompt_hash} 'BEGIN{printf "%0.6f", (ps>0)?(1-(sqrt((ps-rs)^2)/ps)):1}')

        # Final Weighted Score
        score=$(awk -v s1="$sim" -v s2="$recency_score" -v s3="$size_score" -v w1="$w_sim" -v w2="$w_rec" -v w3="$w_size" \
            'BEGIN{printf "%0.6f", w1*s1 + w2*s2 + w3*s3}')
        
        log_debug "Candidate: hash='$db_prompt_hash', dist=$dist, score=$score"

        if (( $(awk -v a="$score" -v b="$best_score" 'BEGIN{print (a>b)?1:0}') )); then
            best_score=$score
            best_response="$response_ref"
            log_debug "New best candidate found with score $best_score"
        fi
    done <<< "$memories"

    echo "$best_response"
}

# ---------------- TRIUMVIRATE ----------------
run_triumvirate_agent_fast(){
    local prompt="$1"

    local cached_response_ref
    cached_response_ref=$(get_weighted_cached_response "$prompt" 12)
    if [[ -n "$cached_response_ref" ]]; then
        log_success "Found a high-quality match in fuzzy cache."
        echo -e "\nâœ… ${CYAN}Weighted Cached Response:${NC}\n$(retrieve_output_fast "$cached_response_ref")"
        return
    fi
    log_info "No suitable cache entry found. Proceeding with live model inference."

    local pool_info prompt_hash instance_hash
    pool_info=$(setup_task_pool "$prompt")
    prompt_hash=${pool_info%% *}
    instance_hash=$(echo "$pool_info" | awk '{print $2}')

    local current_thought="$prompt"
    local final_response=""

    for ((i=1;i<=MAX_TRIUMVIRATE_ROUNDS;i++)); do
        log_think "Round $i/$MAX_TRIUMVIRATE_ROUNDS - Messenger analyzing context..."
        local messenger_resp
        messenger_resp=$(run_worker_fast "$MESSENGER_MODEL" "You are a thoughtful analyst. Your job is to understand the user's request and provide a clear, structured analysis of the core problem and requirements." "$current_thought")

        log_plan "Round $i/$MAX_TRIUMVIRATE_ROUNDS - Combinator creating a high-level plan..."
        local combinator_resp
        combinator_resp=$(run_worker_fast "$COMBINATOR_MODEL" "You are a master strategist. Based on the analysis, create a step-by-step plan to solve the problem. The plan should be clear and logical." "$messenger_resp")

        log_execute "Round $i/$MAX_TRIUMVIRATE_ROUNDS - Trader executing the plan..."
        local trader_resp
        trader_resp=$(run_worker_fast "$TRADER_MODEL" "You are an expert executor. Take the provided plan and generate the final, complete output. If it's code, write the full, working code. If it's text, write the final answer. Add the tag [FINAL_ANSWER] if you are confident the task is complete." "$combinator_resp")

        final_response="$trader_resp"
        current_thought="$trader_resp" # The output of one round becomes the input for the next
        
        if [[ "$trader_resp" =~ \[FINAL_ANSWER\] ]]; then
            log_success "Trader declared [FINAL_ANSWER]. Concluding workflow."
            break
        fi
        log_info "Round $i complete. Continuing to next round for refinement..."
    done

    local stored_ref
    stored_ref=$(store_output_fast "$final_response")
    add_to_memory_fast "$prompt" "$stored_ref" "$prompt_hash" "$instance_hash"
    echo -e "\nâœ… ${GREEN}Final Response:${NC}\n$(retrieve_output_fast "$stored_ref")"
}

# ---------------- CLI ----------------
main(){
    # Ensure Ollama is running before doing anything else
    if ! pgrep -f "ollama serve" >/dev/null; then
        log_warn "Ollama server is not running. Attempting to start it..."
        if ! command -v "$OLLAMA_BIN" &>/dev/null; then
            log_error "Ollama command not found. Please install Ollama."
        fi
        nohup "$OLLAMA_BIN" serve >/dev/null 2&>1 &
        sleep 3
        if ! pgrep -f "ollama serve" >/dev/null; then
            log_error "Failed to start Ollama server. Please start it manually."
        fi
        log_success "Ollama server started."
    fi

    init_db
    if [[ $# -eq 0 ]]; then
        echo -e "${YELLOW}Usage: $0 '<prompt>'${NC}"
        echo "Example: $0 \"create a python script to organize files in my Downloads folder by file type\""
        exit 1
    fi
    local prompt="$*"
    run_triumvirate_agent_fast "$prompt"
}

if [[ "${BASH_SOURCE[0]}" == "$0" ]]; then
    main "$@"
fi
